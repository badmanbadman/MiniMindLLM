from torch import  nn

#  loss 的入参详细推导过程
"""解析
首先不看形状变化
    input = res.logits
    target = Y
词表: {0:<pad>,1:'汪',2:'婷婷',3:'蒋',4:'斌'}
分别对应：汪，婷婷，蒋，斌， 4个词
假设一个训练批次长度seq_len = 8，总共训练一个批次batch_size = 1，数据为 ‘蒋汪婷婷’
X:即输入的数据如下 X = [
    3,  #蒋
    1,  #汪
    2,  #婷婷
    0,  #填充token 用来补齐序列长度
]

Y:即输出的期望数据如下 Y = [
    1,  #汪
    2,  #婷婷
    0,  #填充token 用来补齐序列长度
    0,  #填充token 用来补齐序列长度
]

X进入model进行计算
    1.1、初始化词嵌入层
        假设向量维度为512维,词表大小为5，
        1.1.1 初始化嵌入的所有可嵌入的tokens  embed_tokens
            embed_tokens = [
                [0.1,0.1,0.1,0.1, ..., 0.1,0.1,0.1,0.1,  0.1,0.1,], #长度为512
                [0.2,0.2,0.2,0.2, ..., 0.2,0.2,0.2,0.2,  0.2,0.2,],
                [0.3,0.3,0.3,0.3, ..., 0.3,0.3,0.3,0.3,  0.3,0.3,],
                [0.4,0.4,0.4,0.4, ..., 0.4,0.4,0.4,0.4,  0.4,0.4,],
                [0.5,0.5,0.5,0.5, ..., 0.5,0.5,0.5,0.5,  0.5,0.5,],
            ]
    1.2、旋转位置编码预计算并缓存
    1.3、初始化model层要用的dropout层
    1.4、初始化本层要使用的归一化层
    1.5、初始化transformer块堆叠，形成深度网络layers
        每个transformer块就是一个独立的层（MiniMindBlock）
            1.5.1 初始化transformer块
                1.5.1.1 注意力头层初始化（虽说是多头注意力，但是我们在对每一层的transformer进行注意力头计算的时候，会在一层计算多个头组成的大矩阵，因此每个transformer块我们只用初始化一个注意力计算层就够了）
                    假设每层注意力头数量8，
                    (1)初始化计算KV的头数：因为这里适配了分组注意力机制，（节约缓存的同时提高了运算速度，注意力结果也没有受到太大的影响）在模型运行前的配置项目中，我们配置num_key_value_heads来启动分组注意力机制
                    注意：初始化对num_key_value_heads进行设置的时候必须能被num_attention_heads整除，因为后面我们要对num_key_value进行复制以匹配计算Qw
                    (2)计算num_key_value_heads的重复次数，
                    (3)计算注意力头的维度：词嵌入维度（隐藏层）除以 注意力头数 【512/8=64】
                    (4)初始化Q,K,V的投影层
                        创建Q(查询)的线性层(全连接层)q_proj，输入为隐藏层维度（因为是对每个词进行计算，每个词被转化为了长度为512的向量），输出为Q的注意力头数量*每个注意力头的维度 8*64
                        创建K(键) 的线性层（全连接层）k_proj,输入为隐藏层维度（也是对每个词进行计算，每个词被转化为了长度为512的向量），输出为K的注意力头数量*每个注意力头的维度 2*64
                        创建V(键) 的线性层（全连接层）v_proj,输入为隐藏层维度（也是对每个词进行计算，每个词被转化为了长度为512的向量），输出为V的注意力头数量*每个注意力头的维度 2*64
                        这里要注意，采用分组注意力机制的时候，K和V的头数少，因此线性层的计算量就会变少, 在GQA 的主要节省来自于生成更少的 K 和 V 张量，
                        注意力头数量*每个注意力头的维度 的含义是将所有的注意力头拼接为一个大的矩阵
                        此时线性层建立的矩阵如下：
                        q_proj.weight = [
                            [w1_1, w1_2, w1_3, ..., w1_512],   # 第1个头的第1个维度权重
                            [w2_1, w2_2, w2_3, ..., w2_512],   # 第1个头的第2个维度权重  
                            ...,
                            [w64_1, w64_2, ..., w64_512],   # 第1个头的第64个维度权重
                            [w65_1, w65_2, ..., w65_512],   # 第2个头的第1个维度权重
                            ...,
                            ...,
                            ...,
                            ...,
                            ...,
                            [w512_1, w512_2, ..., w512_512] # 第8个头的第64个维度权重
                        ]  # [512,512]
                        # 2个头
                        k_proj.weight = [
                            [w1_1, w1_2, w1_3, ..., w1_512],   # 第1个头的第1个维度权重
                            [w2_1, w2_2, w2_3, ..., w2_512],   # 第1个头的第2个维度权重  
                            ...,
                            [w64_1, w64_2, ..., w64_512],   # 第1个头的第64个维度权重
                            [w65_1, w65_2, ..., w65_512],   # 第2个头的第1个维度权重
                            ...,
                            [w128_1, w128_2, ..., w128_512] # 第2个头的第64个维度权重
                        ]  # [128,512]
                        # 2个头
                        v_proj.weight = [
                            [w1_1, w1_2, w1_3, ..., w1_512],   # 第1个头的第1个维度权重
                            [w2_1, w2_2, w2_3, ..., w2_512],   # 第1个头的第2个维度权重  
                            ...,
                            [w64_1, w64_2, ..., w64_512],   # 第1个头的第64个维度权重
                            [w65_1, w65_2, ..., w65_512],   # 第2个头的第1个维度权重
                            ...,
                            [w128_1, w128_2, ..., w128_512] # 第2个头的第64个维度权重
                        ]  # [128,512]

                    (5)初始化注意力层要用的Dropout层
                1.5.1.2、在transformer块中要使用的归一化层初始化
                        创建2个长度为512,初始值为1的张量  [1,1,1,1,1,....1,] 长度为512
                1.5.1.3、初始化前馈网络层，目前我们设计了门控前馈网络层，和专家模型的门控前馈网络层，没有使用传统的前馈网络层
                        (1 )普通门控前馈网络层 初始化
                            (1.1)初始化门控前馈网络层
                               ( 1.1.1) 初始化计算门控前馈网络层的中间层维度，中间层维度为动态计算而来，是隐藏层的 8/3 ≈ 2.666倍，是经验性的比例因子，通过，通过这个倍数*隐藏层维度再向上取整64的倍数，再*64，得出的数就是中间层维度，
                                    64是个对GPU运行优化的计算位数，具体原因可能设计到GPU运行的底层原理，没有深入了解
                                    动态计算而得  中间层维度是 intermediate_size = 64 * 22 = 1408
                                (1.1.2 )初始化3个线性投影层 : gate_proj(门控层) up_proj(升维层) 这个两个维度相同，down_proj(降维层),降维输出
                                    每个线性投影生成初始化一个矩阵
                                        gate_proj.weight = [
                                            [w1_1, w1_2, w1_3, ..., w1_512], #第1个
                                            [w2_1, w2_2, w2_3, ..., w2_512], #第2个
                                            [w3_1, w3_2, w3_3, ..., w3_512], #第3个
                                            ...,
                                            ...,
                                            ...,
                                            [w1408_1, w1408_2, w1408_3, ..., w1408_512], #第1408个
                                        ]  # [512, 1408]  这个矩阵再通过激活函数计算后，作为计算升维后的up_project矩阵
                                        
                                        up_proj.weight = [
                                            [w1_1, w1_2, w1_3, ..., w1_512], #第1个，长度512
                                            [w2_1, w2_2, w2_3, ..., w2_512], #第2个
                                            [w3_1, w3_2, w3_3, ..., w3_512], #第3个
                                            ...,
                                            ...,
                                            ...,
                                            [w1408_1, w1408_2, w1408_3, ..., w1408_512], #第1408个
                                        ] # [512, 1408]  这个矩阵用来保留初始化进来的X的所有信息

                                        down_proj.weight = [
                                            [w1_1, w1_2, w1_3, ..., w1_1408], #第1个，长度1408
                                            [w2_1, w2_2, w2_3, ..., w2_1408], #第2个
                                            [w3_1, w3_2, w3_3, ..., w3_1408], #第3个
                                            ...,
                                            ...,
                                            ...,
                                            [w512_1, w512_2, w512_3, ..., w512_1408], #第512个
                                        ]  #[1408, 512]  这个矩阵用来将升维后的信息进行降维
                                (1.1.3) 初始化Dropout层
                                (1.1.4) 设置激活函数

                        (2 )专家门控网络层  初始化
                               ( 2.1)初始化路由专家网络列表，由模型启动的时候配置的路由专家数量n_routed_experts = 4循环生成，每个路由专家网络层都是一个独立的 普通门控前馈网络层（如上所示），所有路由专家网络层都初始化，独立维护其中的线性矩阵
                                    此时整个路由专家网络层矩阵如下：n_routed_experts * 门控前馈网络层维护的3个矩阵， 此时维护矩阵为4*3 = 12 个，每个门控前馈网络层维护3个矩阵（门控），升维，降维），
                                    4个(gate_proj.weight, up_proj.weight, down_proj.weight)
                                (2.2) 初始化共享专家网络列表：由模型启动的时候配置的共享专家数量 share_experts = 2 循环生成，每个共享专家网络与路由专家网络其中逻辑相同，每个共享专家网络也是独立的 普通门控前馈网络层
                                    2个(gate_proj.weight, up_proj.weight, down_proj.weight)
                                (2.3) 初始化 专家门控网络 gate ，计算所需要参加计算的路由专家
                                    (2.3.1) 初始化配置每个token需要top_k个专家（top_k个普通前馈网络）进行计算，
                                    (2.3.2) 初始化路由专家的总数
                                    (2.3.3) 初始化专家评分的损失函数，比如softmax
                                    (2.3.4) 初始化辅助损失权重
                                    (2.3.5) 初始化门控维度 gating_dim = hidden_size = 512,(因为是对每一个token进行计算，所以就是以token的隐藏层维度来的)
                                    (2.3.6) 初始化空的门控权重矩阵，使用kaiMing算法进行初始化值赋予
                                        这里的门控权重矩阵是一个[512,4]的矩阵，
                                        gate_weight = [
                                            [w1_1, w1_2, w1_3, ..., w1_512],  # 专家1的门控权重
                                            [w2_1, w2_2, w2_3, ..., w2_512],  # 专家2的门控权重
                                            [w3_1, w3_2, w3_3, ..., w3_512],  # 专家3的门控权重
                                            [w4_1, w4_2, w4_3, ..., w4_512],  # 专家4的门控权重
                                        ]
                                        
                                        每一行代表一个专家门控权重向量，比如
                                        w1_1就是专家1对本次计算的token的第一维的门控权重，
                                        w1_2是专家1对本次计算的token的第二维的门控权重，
                                        依次类推，
                                        w1_512是专家1对本次计算的token的第512维的门控权重，
【
至此，所有初始化工作完成，
假设
词表大小为5
Transformer层数 为 8
注意力头数为 8
分组注意力用的 kv 为2
路由专家为4
共享专家为2
每个词向量维度为512，

目前形成的可学习的参数矩阵有：
embed_tokens: 词表形成的词嵌入矩阵 [5, 512]
8个transformer块:
    每个块 8个q_proj.weight,2个k_proj.weight,2个v_proj.weight 12个小矩阵，根据QKV，分组后合并在3个大矩阵中，Qw形状为[512,512],Kw形状为[128,512]，Vw形状为[128,512]
    如果使用普通门控前馈网络层
        维护3个矩阵，分别为gate_proj.weight:形状为[512, 1408] (门控),up_proj.weight：[512, 1408] (升维丰富信息)，down_proj.weight[1408, 512] (降维输出)
    如果使用专家模型前馈网络层
        维护4个路由专家（普通门控前馈网络层）： 共4*3 = 12个矩阵，矩阵形状如上面说的普通门控前馈网络层的3个矩阵
        维护2个共享专家（普通门控前馈网络层）： 共2*3 = 6个矩阵，矩阵形状如上面说的普通门控前馈网络层的3个矩阵
        维护1个专家门控矩阵，形状为[512,4]
        共计19个矩阵
】


model前向传播
    1、词嵌入层对输入的input_ids进行初始化，将input_ids 也就是tokenIds转化为密集向量记为hidden_states
        X = [[3,1,2,0 ]] 
        hidden_states = [[  [0.4,0.4,0.4,0.4, ..., 0.4,0.4,0.4,0.4,  0.4,0.4,],     #对应 input_id  3
                            [0.2,0.2,0.2,0.2, ..., 0.2,0.2,0.2,0.2,  0.2,0.2,],     #对应 input_id  1
                            [0.3,0.3,0.3,0.3, ..., 0.3,0.3,0.3,0.3,  0.3,0.3,],     #对应 input_id  2
                            [0.1,0.1,0.1,0.1, ..., 0.1,0.1,0.1,0.1,  0.1,0.1,] ]   ]#对应 input_id  0
    1、采用dropout随机丢弃一些数据(这个数据就是本次训练的原始数据原始的 hidden_states)
        目前假设不丢弃（方便计算）
        hidden_states = [[  [0.4,0.4,0.4,0.4, ..., 0.4,0.4,0.4,0.4,  0.4,0.4,],
                            [0.2,0.2,0.2,0.2, ..., 0.2,0.2,0.2,0.2,  0.2,0.2,],
                            [0.3,0.3,0.3,0.3, ..., 0.3,0.3,0.3,0.3,  0.3,0.3,],
                            [0.1,0.1,0.1,0.1, ..., 0.1,0.1,0.1,0.1,  0.1,0.1,]]]
    2、根据预计算好的位置编码矩阵，对输入的序列进行位置编码矩阵获取，方便传入注意力层进行位置编码计算

    3、循环执行transformer块
        进入transformer块的前向传播
            入参1：为model层前向传播进来的原始的hidden_states
            入参2：预计算好的编码位置矩阵
            入参3：上一层训练传递下来的KV矩阵，(可选)
            入参4：注意力掩码矩阵（可选）
        3.1 进行第一次残差连接，也就是将传递下来的hidden_states（原始值）进行保存,当作残差，
        3.2 残差进行归一化（会使计算更更加稳定）
        3.3 进入注意力层
                入参1：归一化后的残差（原始值）
                入参2：预计算好的编码位置矩阵
                入参3：上一层训练传递下来的KV矩阵，(可选)
                入参4：注意力掩码矩阵（可选）
           注意力层前向传播
              (1)接收到输入的参数1,也就是上一层传下来的归一化后的残差（原始值），
                根据x的shape，解构计算出 批次大小，序列长度
              
              (2)将x投影到上面初始化好的q_proj,k_proj,v_proj，3个线性层，将x与这三个线性层的矩阵计算
                x的形状为[4,512]
                Qw形状为[512,512],Kw形状为[128,512]，Vw形状为[128,512]
                投影计算后得到 
                xq = x @ Qw^T -> [4,512] @ [512,512] = [4,512]
                xk = x @ Kw^T -> [4,512] @ [512,128] = [4,128]
                xv = x @ Vw^T -> [4,512] @ [512,128] = [4,128]

              (3)然后将Q,K,V三个大矩阵进行重塑，Q变为8个小矩阵，K变为2个小矩阵，V变为2个小矩阵，每个小矩阵的维度 是64维
                重塑后的形状为
                xq = [1,4,8,64] #一个批次，每个批次的长度为4，每个批次的每个token都有一个8*64的矩阵
                xk = [1,4,2,64] #一个批次，每个批次的长度为4，每个批次的每个token都有一个2*64的矩阵
                xv = [1,4,2,64] #一个批次，每个批次的长度为4，每个批次的每个token都有一个2*64的矩阵

              (4)对本批次的数据根据传进来的预计算好的编码位置矩阵，进行位置编码，让xq，xk,xv之间计算的时候会呈现出位置相关性，
              (5)对xk,xv进行复制拼接，由2个头转化为8个头，方便与8个头的xq进行计算
                此时xq = [1,4,8,64] #一个批次，每个批次的长度为4，每个批次的每个token都有一个8*64的矩阵
                    xk = [1,4,8,64]
                    xv = [1,4,8,64]
                进行转置：xq = [1,8,4,64]  #一个批次，8个头，每个头有4个
                         xk = [1,8,4,64]
                         xv = [1,8,4,64]
                计算注意力分数：xq与xk进行矩阵乘法，在除以维度等计算，
                注意力分数与xv矩阵进行乘法，输出经过注意力层计算后的数据
                Attention的输出是：每个token的上下文感知的新表示
                对比输入输出
                输入：
                # 每个token的原始表示（不考虑上下文）
                token0_vec = [原始特征0, 原始特征1, ...]  # 只包含token自身信息
                token1_vec = [原始特征0, 原始特征1, ...]
                输出：
                # 每个token的上下文感知表示
                token0_new_vec = [上下文特征0, 上下文特征1, ...]  # 融合了序列中所有相关信息
                token1_new_vec = [上下文特征0, 上下文特征1, ...]
                形状没有发生变化
                输出hidden_state
        3.4 从注意力层出来，得到了一个新包含有上下文特征的新的hidden_state,
            第二次残差连接：将上面3.1的数值与本次输出的数据进行相加形成新的残差
        3.5 进入前馈网络层前向传播
            将二次连接的残差归一化当作 前馈网络层的前向传播入参x
            3.5.1 进入普通门控前馈网络层前向传播
                (1)对x进行门控投影,即
                 gate = gate_proj(x) = x@gate_proj.weight^T = [4,512] @[1408,512]^T = [4, 1408]
                (2)对x进行升维,即
                 up = up_proj(x) = x@ up_proj.weight^T = [4,512] @ [1408,512]^T = [4,1408]
                (3)使用激活函数对门控投影进行计算,决定哪些权重保留/抑制
                 activate_gate = act_fu(gate)
                (4)对升维后的数据进行门控计算使用元素乘法
                 gate_out = activate_gate * gate 
                 得到门控计算后的矩阵,
                (5)对门控计算后的矩阵进行降维
                 down = down_proj(x) = up @ down_proj.weight = [4,1408] @[1408,512] = [4,512]
                
                (6)进入dropout正则化层丢弃点
                (7) 输出正则化后的结果dropout(down)
            3.5.2 进入专家门控前馈网络层

              



                

    
    对logits进行形状变化:
"""
def calcLoss():
    # res 是经过前向传播model之后的返回值，包含了对每个批次中每个token的预测得分
    # Y是期望的值，（在词表中的对应的indexs）
    res = None
    Y = None
    # 交叉熵损失函数，算出的值越小越好，入参有2个
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    input = res.logits.view(-1, res.logits.size(-1))
    target = Y.view(-1)


    # 总损失
    loss = loss_fct(input, target).view(Y.size())