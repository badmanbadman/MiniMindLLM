"""ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜MiniMind configğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜"""
"""
transformers: Hugging Face çš„Transformsåº“ï¼Œæä¾›äº†å„ç§ä¸è®­ç»ƒæ¨¡å‹å’Œå·¥å…·
PretrainedConfig: æ‰€æœ‰ä¸è®­ç»ƒæ¨¡å‹é…ç½®çš„åŸºç±»ï¼Œæä¾›äº†åºåˆ—åŒ–ï¼Œä¿å­˜ï¼ŒåŠ è½½ç­‰é€šç”¨çš„åŠŸèƒ½
"""
from transformers import PretrainedConfig

class MiniMindConfig(PretrainedConfig):
    # æ ¹æ®è¿™ä¸ªå­—æ®µçŸ¥é“åº”è¯¥åŠ è½½å“ªä¸ªå…·ä½“çš„æ¨¡å‹ç±»
    model_type = "minimind"

    """
    MoE å·¥ä½œæµç¨‹æ¯”å–»
    æƒ³è±¡ä¸€ä¸ªåŒ»é™¢åˆ†è¯Šç³»ç»Ÿï¼š
    n_routed_experts = 4 â†’ æœ‰4ä¸ªä¸“ç§‘åŒ»ç”Ÿ(å¿ƒå†…ç§‘ã€ç¥ç»ç§‘ç­‰)
    num_experts_per_tok = 2 â†’ æ¯ä¸ªç—…äººåŒæ—¶çœ‹2ä¸ªç›¸å…³ä¸“ç§‘
    n_shared_experts = 1 â†’ è¿˜æœ‰1ä¸ªå…¨ç§‘åŒ»ç”Ÿå¤„ç†æ‰€æœ‰ç—…ä¾‹
    scoring_func = 'softmax' â†’ æ ¹æ®ç—‡çŠ¶è¯„åˆ†å†³å®šåˆ†è¯Šåˆ°å“ªä¸ªç§‘å®¤
    """
    def __init__(
            self,
            # æ ¸å¿ƒTransformerå‚æ•°
            dropout: float = 0.0,           # Dropoutæ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
            bos_token_id: int = 1,          #åºåˆ—å¼€å§‹æ ‡è®°çš„ID
            eos_token_id: int = 2,          #åºåˆ—ç»“æŸæ ‡è®°çš„ID
            hidden_act: str = 'silu',       #éšè—å±‚çš„æ¿€æ´»å‡½æ•°ï¼Œsiluæ¯”RelUæ›´åŠ å¹³æ»‘
            hidden_size: int = 512,         #éšè—å±‚ç»´åº¦ï¼Œæ¯ä¸ªtokençš„å‘é‡å¤§å°
            intermediate_size: int = None,  #FFNä¸­é—´å±‚ç»´åº¦ï¼ˆå‰é¦ˆç½‘ç»œç»´åº¦ï¼‰ï¼Œé€šå¸¸å¸ˆhidden_sizeçš„4å€æ•°
            max_position_embeddings: int = 32768, #æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦
            num_attention_heads: int = 8,   #æ³¨æ„åŠ›å¤´æ•°
            num_hidden_layers: int = 8,     #Transformerå±‚æ•°
            num_key_value_heads: int = 2,   #ç”¨äºåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰çš„keyå’Œvalueçš„æ³¨æ„åŠ›å¤´æ•°ã€‚å¦‚æœä¸num_attention_headsä¸åŒï¼Œåˆ™ä½¿ç”¨GQA
            vocab_size: int =6400,          #è¯æ±‡è¡¨çš„å¤§å°
            # å½’ä¸€åŒ–å’Œä½ç½®ç¼–ç 
            rms_norm_eps: float = 1e-05,    #RMSNormçš„epsilonå€¼ï¼Œç”¨äºæ•°å€¼ç¨³å®šæ€§ é˜²æ­¢é™¤é›¶é”™è¯¯ æ¢¯åº¦ç¨³å®šæ€§
            rope_theta: int = 1000000.0,    #RoPE(æ—‹è½¬ä½ç½®ç¼–ç )çš„thetaå‚æ•°ï¼Œ ç¡®ä¿åœ¨é•¿åºåˆ—ä¸­ä¸åŒä½ç½®æœ‰è¶³å¤Ÿçš„åŒºåˆ†åº¦
            inference_rope_scaling: bool = False,# æ˜¯å¦åœ¨æ¨ç†æ—¶å€™ä½¿ç”¨RoPEç¼©æ”¾ï¼ˆç”¨äºæ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ï¼Œå¦‚æœé¢å¤–iTrueï¼Œåˆ™è®¾ç½®ä¸€ä¸ªRoPEç¼©æ”¾é…ç½®å­—å…¸
            flash_attn: bool = True,        #æ˜¯å¦ä½¿ç”¨Flash Attentionï¼Œ*(ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ï¼Œæ›´å¿«çš„å†…å­˜è®¿é—®)
            # MoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®  use_moeä¸ºfalseçš„æ—¶å€™ä¸‹é¢é…ç½®ä¸ç”Ÿæ•ˆ
            use_moe: bool = False,          #æ˜¯å¦å¯ç”¨MoE
            num_experts_per_tok: int = 2,   #æ¯ä¸ªtokenä½¿ç”¨å‡ ä¸ªä¸“å®¶
            n_routed_experts: int = 4,      #è·¯ç”±ä¸“å®¶çš„æ€»æ•°
            n_shared_experts: int = 1,      # å…±äº«ä¸“å®¶çš„æ•°é‡ï¼ˆæ‰€æœ‰tokenéƒ½ä¼šä½¿ç”¨ï¼‰
            scoring_func: str = 'softmax',  #ä¸“å®¶é€‰æ‹©è¯„åˆ†å‡½æ•°
            aux_loss_alpha: float = 0.1,    #è¾…åŠ©æŸå¤±æƒé‡ï¼ˆå¹³è¡¡ä¸“å®¶ä½¿ç”¨ï¼‰
            seq_aux: bool = True,           #åºåˆ—çº§è¾…åŠ©æŸå¤±è®¡ç®—
            norm_topk_prob: bool = True,    #æ˜¯å¦å¯¹top-kæ¦‚ç‡å½’ä¸€åŒ–
            **kwargs
    ):      
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        self.flash_attn = flash_attn
        # å¤–æ¨é•¿åº¦ = factor*original_max_position_embeddings
        self.rope_scaling = {
            'beta_fast': 4, # å¿«é€Ÿbetaå‚æ•°(YaRNæ–¹æ³•)
            'beta_slow': 1, # æ…¢é€Ÿbetaå‚æ•°
            'factor': 4,    # ç¼©æ”¾å› å­
            'original_max_position_embeddings': 2048, # åŸå§‹è®­ç»ƒé•¿åº¦
            'type': 'yarn'  # ç¼©æ”¾ç±»å‹ï¼šYaRNæ–¹æ³•
        } if self.inference_rope_scaling else None

        # MoEï¼ˆæ··åˆä¸“å®¶ï¼‰é…ç½®  use_moeä¸ºfalseçš„æ—¶å€™ä¸‹é¢é…ç½®ä¸ç”Ÿæ•ˆ
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts        #æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts        #å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func                #è¯„åˆ†å‡½æ•°ï¼Œ é»˜è®¤ä¸ºâ€˜softmaxâ€™
        self.aux_loss_alpha = aux_loss_alpha            #è¾…åŠ©æŸå¤±å‡½æ•°çš„alphaå‚æ•°
        self.seq_aux = seq_aux                          #æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob            #æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡


"""ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜MiniMind ModelğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜"""
import math
import torch
"""torch.nn.initæä¾›äº†ï¼š
1ã€å¤šç§åˆå§‹åŒ–çš„æ–¹æ³•:é€‚åº”ä¸åŒçš„ç½‘ç»œæœºæ„å’Œæ¿€æ´»å‡½æ•°
2ã€è®­ç»ƒç¨³å®šæ€§:é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
3ã€æ”¶æ•›åŠ é€Ÿ: é€‚åˆçš„åˆå§‹åŒ–è®©æ¨¡å‹æ›´å¿«æ”¶æ•›
4ã€ä¸“ä¸šé…ç½®: é’ˆå¯¹Transformerï¼ŒMoEç­‰ç‰¹å®šç»“æ„çš„ä¼˜åŒ–åˆå§‹åŒ–
"""
import torch.nn.init as init
"""torch.nn.functionalæä¾›äº†å¤§é‡çš„æ— çŠ¶æ€å‡½æ•°å¦‚ï¼š
1ã€æ¿€æ´»å‡½æ•°
2ã€æ³¨æ„åŠ›æœºåˆ¶
3ã€dropoutå’Œå½’ä¸€åŒ–
4ã€æŸå¤±å‡½æ•°
5ã€é«˜æ•ˆè®¡ç®—æ“ä½œï¼Œå¦‚top-ké€‰æ‹©ï¼ŒçŸ©é˜µæ“ä½œç­‰
6ã€ä¼˜åŒ–å‡½æ•°ï¼šFlash Attentionï¼Œ
è¿™ä¸ªæ¨¡å—æä¾›äº†æ„å»ºç°ä»£Transformerå’ŒMoEæ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰åŸºç¡€å‡½æ•°å¼ç»„ä»¶
"""
import torch.nn.functional as F
"""torch.nnç¥ç»ç½‘ç»œæ¨¡å—ï¼Œæä¾›äº†æ„å»ºç¥ç»ç½‘ç»œæ‰€éœ€çš„æ‰€æœ‰åŸºç¡€ç»„ä»¶ï¼ŒåŒ…å«äº†ï¼š
1ã€æ¨¡å—æ„æ¶åŸºç¡€ï¼šnn.moduleåŸºç±»
2ã€ä¸°å¯Œçš„å±‚ç±»å‹ï¼šçº¿æ€§å±‚ï¼ŒåµŒå…¥å±‚ï¼Œå½’ä¸€åŒ–å±‚ç­‰ç­‰
3ã€ç»„ç»‡å®¹å™¨ï¼š moduleListï¼ŒSequentialï¼ŒModuleDict
4ã€å‚æ•°ç®¡ç†ï¼šè‡ªåŠ¨è·Ÿè¸ªï¼Œè®¾å¤‡ç§»åŠ¨ï¼ŒçŠ¶æ€ä¿å­˜
5ã€è®­ç»ƒå·¥å…·ï¼šæ¨¡å¼åˆ‡æ¢ï¼Œæ¢¯åº¦ç®¡ç†
"""
from torch import nn
"""ACT2FNæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå°†å­—ç¬¦ä¸²æ ‡è¯†ç¬¦æ˜ å°„åˆ°å¯¹åº”çš„æ¿€æ´»å‡½æ•°
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„æ¿€æ´»å‡½æ•°
print("Available activation functions:")
for act_name in ACT2FN.keys():
    print(f"  - {act_name}")

# å…¸å‹è¾“å‡ºï¼š
# Available activation functions:
#   - gelu
#   - gelu_new
#   - gelu_fast
#   - quick_gelu
#   - gelu_python
......
egï¼šä½¿ç”¨é…ç½®ä¸­çš„æ¿€æ´»å‡½æ•°
self.activation = ACT2FN[config.hidden_act]
"""
from transformers.activations  import ACT2FN
"""Unionç±»å‹: è”åˆç±»å‹,  ç”¨äºè¡¨ç¤ºä¸€ä¸ªå€¼å¯ä»¥æ˜¯å‡ ç§ç±»å‹ä¸­çš„ä¸€ç§ã€‚
ä¾‹å¦‚ï¼ŒUnion[int, str]è¡¨ç¤ºä¸€ä¸ªå€¼å¯ä»¥æ˜¯æ•´æ•°æˆ–å­—ç¬¦ä¸²
"""
from typing import Optional, Tuple, List, Union
"""PreTrainedModelæ˜¯Hugging Face Transformersåº“ä¸­å›½æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç±»ï¼Œæä¾›äº†æ¨¡å‹ç®¡ç†çš„åŸºç¡€è®¾æ–½
1ã€æ¨¡å‹åºåˆ—åŒ–åŠŸèƒ½ï¼ˆå°†æ¨¡å‹ä¿å­˜ä¸ºæ–‡ä»¶ï¼ŒåŠ è½½æ¨¡å‹æ–‡ä»¶ï¼‰
2ã€å‚æ•°ç®¡ç†
3ã€ä¸é…ç½®ç±»é›†æˆ
GeneerationMixinæ··å…¥ç±»
æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ï¼š ä¸ºæ¨¡å‹æ·»åŠ äº†å„ç§æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼Œæ˜¯æ„å»ºç”Ÿæˆå¼è¯­è¨€æ¨¡å‹çš„å…³é”®
"""
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
"""CausalLMOutputWithPastæä¾›äº†ï¼š
1ã€æ ‡å‡†åŒ–çš„è¾“å‡ºæ ¼å¼ï¼šä¸hugging Faceç”Ÿæ€ç³»ç»Ÿå…¼å®¹
2ã€ç”Ÿæˆä¼˜åŒ–ï¼šåŒ…å«past_key_valuesç”¨äºé«˜æ•ˆçš„è‡ªå›å½’ç”Ÿæˆ
3ã€è®­ç»ƒæ”¯å‡ºï¼šåŒ…å«æŸå¤±è®¡ç®—å’Œæ¢¯åº¦åå‘ä¼ æ’­æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
4ã€è°ƒè¯•èƒ½åŠ›ï¼šæä¾›éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡çš„è®¿é—®
5ã€æ‰©å±•æ€§ï¼šæ”¯æŒMoEç­‰ç‰¹æ®Šæ¶æ„çš„é¢å¤–è¾“å‡ºå­—æ®µ
è¿™ä¸ªè¾“å‡ºç±»å¯ä»¥è®©å› æœè¯­è¨€æ¨¡å‹èƒ½å¤Ÿæ— ç¼é›†æˆå¤§Transformerçš„è®­ç»ƒã€è¯„ä¼°å’Œç”Ÿæˆç®¡é“ä¸­
"""
from transformers.modeling_outputs import CausalLMOutputWithPast
from .model_minimind_module import RMSNorm,  precompute_freqs_cis,apply_rotary_pos_emb, repeat_kv

class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        #åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰
        """
        åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›GQA
        ç›®çš„ï¼šæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¾ç¤ºçš„æŒ‡å®šäº†num_key_value_heads(KV)å¤´æ•°é‡
        å¦‚æœä¸ºNoneï¼šä½¿ç”¨ä¸queryå¤´ç›¸åŒçš„æ•°é‡ï¼ˆqkvå¤´æ•°ç›¸ç­‰ï¼‰ï¼ˆä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›ï¼‰
        å¦‚æœæœ‰å€¼ï¼šä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„KVå¤´æ•°é‡ï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰
        """
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        assert args.num_attention_heads % self.num_key_value_heads == 0  # å¿…é¡»æ•´é™¤

        self.n_local_heads = args.num_attention_heads #æŸ¥è¯¢å¤´æ•°
        self.n_local_kv_heads = self.num_key_value_heads # é”®å€¼å¤´æ•°


        self.n_rep = self.n_local_heads // self.n_local_kv_heads # é‡å¤æ¬¡æ•°

        #ç»´åº¦é…ç½®
        self.head_dim = args.hidden_size // args.num_attention_heads

        # æŠ•å½±å±‚
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim,bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim,bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)

        # Dropout
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout

        # flashæ”¯æŒ
        self.flash = hasattr(torch.nn.functional,'scaled_dot_product_attention') and args.flash_attn

        # è¾“å…¥æŠ•å½±å’Œé‡å¡‘
    def forward(self,x,position_embeddings, past_key_value=None,use_cache= False, attention_mask=None):
            bsz, seq_len,_ = x.shape

            # æŠ•å½±åˆ°Qã€Kã€V

            xq,xk,xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)

            # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
            xq = xq.view(bsz,seq_len,self.n_local_heads,self.head_dim)
            xk = xk.view(bsz,seq_len,self.n_local_kv_heads,self.head_dim)
            xv = xv.view(bsz,seq_len,self.n_local_kv_heads,self.head_dim)

            # æ—‹è½¬ä½ç½®ç¼–ç åº”ç”¨
            # è·å–é¢„è®¡ç®—çš„coså’Œsin
            cos,sin = position_embeddings
            # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆåªå¯¹å‰seq_lenä¸ªä½ç½®ï¼‰
            xq,xk = apply_rotary_pos_emb(xq,xk,cos[:seq_len], sin[:seq_len])

            # é”®å€¼ç¼“å­˜æœºåˆ¶
            # KVç¼“å­˜å®ç°ï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰
            if past_key_value is not None:
                # æ‹¼æ¥å†å²ç¼“å­˜å’Œå½“å‰é”®å€¼
                xk = torch.cat([past_key_value[0],xk],dim=1) #åœ¨åºåˆ—ç»´åº¦æ‹¼æ¥
                xv = torch.cat([past_key_value[1],xv],dim=1)

            past_kv = (xk, xv) if use_cache else None 

            # GQA å¤„ç†å’Œè½¬ç½®
            # è½¬ç½®å¹¶é‡å¤é”®å€¼å¤´ä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°é‡
            xq,xk,xv = (
                xq.transpose(1,2),
                repeat_kv(xk, self.n_rep).transpose(1,2),
                repeat_kv(xv, self.n_rep).transpose(1,2)
            )

            # åŒè·¯å¾„æ³¨æ„åŠ›è®¡ç®—
            # 1ã€Flash Attentionè·¯å¾„
            if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
                # å‡†å¤‡Flash Attentionéœ€è¦çš„æ©ç 
                attn_mask = (
                    None
                    if attention_mask is None
                    else attention_mask.view(bsz,1,1,-1).expand(bsz,self.n_local_heads,seq_len,-1).bool()
                )
                # ä½¿ç”¨pytorchçš„é«˜æ•ˆæ³¨æ„åŠ›ï¼ˆqkvä¹‹é—´çš„çŸ©é˜µè®¡ç®—é€»è¾‘ï¼ŒåŸç†ä¸æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—ä¸€æ ·ï¼Œä½†æ˜¯åšäº†å¾ˆå¤šä¼˜åŒ–ï¼‰
                output = F.scaled_dot_product_attention(
                    xq,xk,xv,
                    attn_mask=attn_mask, 
                    dropout_p=self.dropout if self.training else 0.0, 
                    is_causal=True # è‡ªåŠ¨åº”ç”¨å› æœæ©ç 
                    )
            else:
                # æ ‡å‡†æ³¨æ„åŠ›è·¯å¾„
                # æ‰‹åŠ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°   è½¬ç½®æ˜¯ä¸ºäº†é‡ç»„Kçš„ç»´åº¦ï¼Œä½¿å…¶èƒ½ä¸Qè¿›è¡Œæœ‰æ„ä¹‰çš„çŸ©é˜µä¹˜æ³•
                scores = (xq @ xk.transpose(-2,-1)) / math.sqrt(self.head_dim)
                # åº”ç”¨å› æœæ©ç ï¼ˆé˜²æ­¢å…³æ³¨æœªæ¥ä½ç½®ï¼‰ã€åˆ›å»ºä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µã€‘
                scores = scores + torch.triu(
                    torch.full((seq_len,seq_len), float('-inf'),device=scores.device),
                    diagonal=1
                ).unsqueeze(0).unsqueeze(0)  #scores + mask

                # åº”ç”¨é¢å¤–çš„æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚å¡«å……ç ï¼‰
                if attention_mask is not None:
                    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                    extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                    scores = scores + extended_attention_mask
                
                # è®¡ç®—æ³¨æ„åŠ›æƒé‡
                scores = F.softmax(scores.float(), dim=1).type_as(xq)
                scores = self.attn_dropout(scores)

                # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼
                output = scores @ xv
            
            # è¾“å‡ºå¤„ç†
            output = output.transpose(1,2).reshape(bsz,seq_len, -1)

            # è¾“å‡ºæŠ•å½±å’Œæ®‹å·®dropout
            output = self.resid_dropout(self.o_proj(output))
            """
            Attentionçš„è¾“å‡ºæ˜¯ï¼šæ¯ä¸ªtokençš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ–°è¡¨ç¤º
            å¯¹æ¯”è¾“å…¥è¾“å‡º
            è¾“å…¥ï¼š
            # æ¯ä¸ªtokençš„åŸå§‹è¡¨ç¤ºï¼ˆä¸è€ƒè™‘ä¸Šä¸‹æ–‡ï¼‰
            token0_vec = [åŸå§‹ç‰¹å¾0, åŸå§‹ç‰¹å¾1, ...]  # åªåŒ…å«tokenè‡ªèº«ä¿¡æ¯
            token1_vec = [åŸå§‹ç‰¹å¾0, åŸå§‹ç‰¹å¾1, ...]
            è¾“å‡ºï¼š
            # æ¯ä¸ªtokençš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤º
            token0_new_vec = [ä¸Šä¸‹æ–‡ç‰¹å¾0, ä¸Šä¸‹æ–‡ç‰¹å¾1, ...]  # èåˆäº†åºåˆ—ä¸­æ‰€æœ‰ç›¸å…³ä¿¡æ¯
            token1_new_vec = [ä¸Šä¸‹æ–‡ç‰¹å¾0, ä¸Šä¸‹æ–‡ç‰¹å¾1, ...]
            """

            return output, past_kv

class FeedForward(nn.Module):
    # é—¨æ§å‰é¦ˆç½‘ç»œ
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        # åŠ¨æ€è®¡ç®—ä¸­é—´å±‚ç»´åº¦
        """
        8/3 â‰ˆ 2.666 æ˜¯ç»éªŒæ€§çš„æ¯”ä¾‹å› å­
        æ„å‘³ç€ä¸­é—´å±‚ç»´åº¦å¤§çº¦æ˜¯éšè—å±‚çš„2.67å€
        config.intermediate_size = 64*((intermeditate_size + 64 -1) // 64)
        å‘ä¸Šå–æ•´åˆ°64å€æ•°çš„ç»å…¸ç®—æ³•
        å…·ä½“è®¡ç®—ç¤ºä¾‹ï¼š
        ç¤ºä¾‹1ï¼šhidden_size = 512
        æ­¥éª¤1-è®¡ç®—åŸºç¡€å€¼
        intermediate_size = int(512*8/3) = int(1365.33) = 1365
        æ­¥éª¤2-å‘ä¸Šå–æ•´åˆ°64çš„å€æ•°
        (1365 + 64 -1) // 64 = (1428)//64 = 22
        64*22 = 1408
        """
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 /3)
            config.intermediate_size = 64*((intermediate_size + 64 -1) // 64)
        
        # ä¸‰ä¸ªæŠ•å½±å±‚
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size,bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size,bias=False)
        self.up_proj = nn.Linear(config.hidden_size,config.intermediate_size,bias=False)

        # dropoutæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(config.dropout)
        # æ¿€æ´»å‡½æ•°
        self.act_fn = ACT2FN[config.hidden_act]
    
    def forward(self,x):
        """é—¨æ§FFN
        ä¼ ç»ŸFFN vs é—¨æ§FFN
        ä¼ ç»ŸFFN: output = activation(linear1(x))  #æ‰€æœ‰ç‰¹å¾åŒç­‰å¤„ç†
        é—¨æ§FFN: output = activation(linear1(x)) * linear2(x) # åŠ¨æ€æ§åˆ¶ä¿¡æ¯æµ
        é—¨æ§çš„ä¼˜åŠ¿ï¼š
        åŠ¨æ€ç‰¹å¾é€‰æ‹©ï¼š
        1ã€gate_outputå†³å®šå“ªäº›ç‰¹å¾åº”è¯¥è¢«å¼ºè°ƒæˆ–è€…æŠ‘åˆ¶
        ç±»ä¼¼ä¸æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½†æ˜¯åœ¨ç‰¹å¾çº§åˆ«
        2ã€æ›´åŠ ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›
        å¯ä»¥å­¦ä¹ å¤æ‚çš„ç‰¹å¾äº¤äº’
        æ¯”ç®€å•çš„å‰é¦ˆç½‘ç»œæœ‰æ›´å¼ºçš„éçº¿æ€§èƒ½åŠ›
        3ã€è®­ç»ƒç¨³å®šæ€§
        é—¨æ§æœºåˆ¶å¯ä»¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼ˆå‚è€ƒç†è§£LSTMé—¨æ§æœºåˆ¶ï¼‰
        è®©ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ æ’ç­‰æ˜ å°„ï¼ˆå½“gateâ‰ˆ1æ—¶ï¼‰ï¼Œè¾“å‡ºçš„ä¿¡æ¯åŸºæœ¬ä¿æŒä¸å˜
        """
        # return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))
        # 1ã€é—¨æ§æŠ•å½±,xçš„ç»´åº¦æ˜¯512ï¼Œæ˜¯ç”±æ³¨æ„åŠ›å±‚çš„8ä¸ª64ç»´çš„çŸ©é˜µæ‹¼æ¥è€Œæˆçš„512ç»´ 
        gate = self.gate_proj(x)

        # 2ã€ä¸ŠæŠ•å½±ï¼ˆå‡é«˜ç»´åº¦ï¼‰ï¼Œè¿™ä¸ªxä¹Ÿæ˜¯512ç»´ï¼Œæ˜¯ç”±æ³¨æ„åŠ›å±‚çš„8ä¸ªå¤´çš„84ç»´æ‹¼æ¥è€Œæˆï¼Œè¿™ä¸ªupç”¨æ¥å°†xå‡é«˜ç»´åº¦ï¼Œä¿å­˜æ›´åŠ ä¸°å¯Œçš„ä¿¡æ¯ï¼Œ
        up = self.up_proj(x)

        # 3ã€æ¿€æ´»å‡½æ•°åº”ç”¨åˆ°é—¨æ§ä¿¡å·ï¼Œè¿™ä¸ªæ˜¯å°†é—¨æ§çŸ©é˜µè¿›è¡Œæ¿€æ´»å‡½æ•°è®¡ç®—ï¼Œåˆ¤æ–­å“ªäº›ä¿¡æ¯éœ€è¦ä¿ç•™å“ªäº›ä¸éœ€è¦ï¼Œ
        # äº§ç”Ÿ0-1ä¹‹é—´çš„é—¨æ§æƒé‡
        # å†³å®šå“ªäº›ç‰¹å¾åº”è¯¥è¢«ä¿ç•™/æŠ‘åˆ¶
        activated_gate = self.act_fn(gate) 

        # 4ã€å…ƒç´ çº§ä¹˜æ³•ï¼ˆé—¨æ§æœºåˆ¶ï¼Œå‡ç»´åä¸é—¨æ§æŠ•å½±æ¿€æ´»åè®¡ç®—ï¼‰ï¼Œå°†é—¨æ§çº¿æ€§çŸ©é˜µç»è¿‡æ¿€æ´»å‡½æ•°è®¡ç®—åçš„å€¼ä¸å‡ç»´åçš„å€¼è¿›è¡Œè®¡ç®—ï¼Œå€¼å°±æ˜¯è¦ä¿ç•™çš„å€¼ï¼Œ
        gate_output = activated_gate * up

        # 5ã€ä¸‹æŠ•å½±å›åŸå§‹ç»´åº¦ï¼Œå°†è¦ä¿ç•™çš„å€¼è¿›è¡Œé™ç»´ï¼Œ
        down = self.down_proj(gate_output)

        # 6ã€Dropoutæ­£åˆ™åŒ–
        output = self.dropout(down)
        return output

class MoEGate(nn.Module): 
    def __init__(self, config: MiniMindConfig):
        super().__init__() 
        self.config = config

        # ä¸“å®¶é€‰æ‹©é…ç½®
        self.top_k = config.num_experts_per_tok #æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = config.n_routed_experts #è·¯ç”±ä¸“å®¶çš„æ€»æ•°

        # è¯„åˆ†å’ŒæŸå¤±é…ç½®
        self.scoring_func = config.scoring_func # ä¸“å®¶è¯„åˆ†å‡½æ•°ç±»å‹
        self.aplha = config.aux_loss_alpha #è¾…åŠ©æŸå¤±æƒé‡
        self.seq_aux = config.seq_aux #æ˜¯å¦è®¡ç®—åºåˆ—çº§è¾…åŠ©æŸå¤±

        # æ¦‚ç‡å½’ä¸€åŒ–
        self.norm_topk_prob = config.norm_topk_prob # æ˜¯å¦å½’ä¸€åŒ–topâ€”kæ¦‚ç‡
        self.gating_dim = config.hidden_size #é—¨æ§ç»´åº¦512

        # é—¨æ§æƒé‡
        self.weight = nn.Parameter(torch.empty(self.n_routed_experts, self.gating_dim))
        self.reset_parameters()
    
    def reset_parameters(self)-> None:
        """Kaimingåˆå§‹åŒ–ï¼š
        ä¹Ÿç§°ä¸ºHeåˆå§‹åŒ–
        ç”±ä½•å‡¯æ˜åœ¨2015å¹´æå‡ºï¼Œä¸“é—¨é’ˆå¯¹ReLUæ—æ¿€æ´»å‡½æ•°ä¼˜åŒ–
        è§£å†³äº†æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜

        æ ‡å‡†KaiMingåˆå§‹åŒ–
        å¯¹äºå‡åŒ€åˆ†å¸ƒU(-bound,bound)
        bound = gain*sqrt(3/fan_in)

        åˆå§‹åŒ–ç­–ç•¥çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼šç¡®ä¿ä¿¡å·åœ¨ç½‘ç»œä¸­ä¼ æ’­æ—¶æ–¹å·®ä¿æŒç¨³å®š
        # å‰å‘ä¼ æ’­: y = x @ W
        # å¦‚æœ x çš„æ–¹å·®æ˜¯ Var(x)ï¼ŒW çš„æ–¹å·®æ˜¯ Var(W)
        # é‚£ä¹ˆ y çš„æ–¹å·® â‰ˆ fan_in * Var(x) * Var(W)

        # ä¸ºäº†ä¿æŒæ–¹å·®ç¨³å®šï¼Œéœ€è¦ï¼š
        fan_in * Var(W) = 1
        # å› æ­¤ï¼šVar(W) = 1 / fan_in

        # å†…éƒ¨è®¡ç®—è¿‡ç¨‹ï¼š
        # gain = sqrt(2.0 / (1 + a^2))  # å¯¹äºLeakyReLU
        # ç”±äº a = sqrt(5)ï¼Œæ‰€ä»¥ï¼š
        # gain = sqrt(2.0 / (1 + 5)) = sqrt(2/6) = sqrt(1/3) â‰ˆ 0.577

        # ç„¶åè®¡ç®—è¾¹ç•Œï¼š
        # bound = gain * sqrt(3 / fan_in)
        #       = sqrt(1/3) * sqrt(3 / fan_in)
        #       = sqrt(1 / fan_in)

        ä¿¡æ¯è¾“å…¥é‡
        # fan_in è¶Šå¤§ï¼Œæ„å‘³ç€ï¼š
        # - æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶æ›´å¤šè¾“å…¥ä¿¡æ¯
        # - éœ€è¦æ›´å°çš„æƒé‡æ¥é¿å…è¾“å‡ºçˆ†ç‚¸
        # - å› æ­¤åˆå§‹åŒ–èŒƒå›´åº”è¯¥æ›´å°

        æ¢¯åº¦æµåŠ¨
        # åå‘ä¼ æ’­æ—¶ï¼š
        # âˆ‚Loss/âˆ‚W = âˆ‚Loss/âˆ‚y * x^T

        # fan_in å½±å“æ¢¯åº¦çš„å¤§å°ï¼š
        # æ›´å¤§çš„ fan_in â†’ æ›´å¤šé¡¹æ±‚å’Œ â†’ å¯èƒ½æ¢¯åº¦æ›´å¤§
        # éœ€è¦é€šè¿‡åˆå§‹åŒ–æ¥å¹³è¡¡

        ç½‘ç»œæ·±åº¦çš„å½±å“
        # åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼š
        # æ¯å±‚çš„ fan_in é€šå¸¸æ˜¯å‰ä¸€å±‚çš„ fan_out
        # ä¸æ°å½“çš„åˆå§‹åŒ–ä¼šå¯¼è‡´æ¢¯åº¦æŒ‡æ•°çº§å˜åŒ–ï¼ˆæ¶ˆå¤±æˆ–çˆ†ç‚¸ï¼‰
        """
        init.kaiming_uniform(self.weight, a=math.sqrt(5)) 
    
    def forward(self,hidden_states):
        # è¾“å…¥å¤„ç†å’Œè¯„åˆ†è®¡ç®—
        bsz, seq_len, h = hidden_states.shape

        hidden_states = hidden_states.view(-1,h)
        """å±•å¹³è¾“å…¥ [batch_size, seq_len,hidden_size]-> [batch_size*seq_len,hidden_size]
        # è¦ç†è§£æ‰¹æ¬¡ï¼šæ‰¿è½½çš„åºåˆ—çš„ä¸ªæ•°ï¼Œ
        # åºåˆ—ï¼šæ‰¿è½½çš„æ˜¯tokençš„ä¸ªæ•°
        # view: è¦æ±‚å†…å­˜è¿ç»­ï¼Œé€Ÿåº¦å¿«
        hidden_states.view(-1, h)

        # reshape: è‡ªåŠ¨å¤„ç†å†…å­˜è¿ç»­æ€§ï¼Œæ›´å®‰å…¨ä½†ç¨æ…¢  
        hidden_states.reshape(-1, h)


        # å‡è®¾åŸå§‹å½¢çŠ¶ï¼š
        hidden_states.shape = [2, 8, 512]  # [bsz, seq_len, h]
        å˜æ¢å‰
        # æ•°æ®åœ¨å†…å­˜ä¸­çš„é€»è¾‘ç»“æ„ï¼š
        [
            # æ‰¹æ¬¡0
            [[token0_0, token0_1, ..., token0_511],  # åºåˆ—ä½ç½®0
            [token1_0, token1_1, ..., token1_511],  # åºåˆ—ä½ç½®1
            ...,
            [token7_0, token7_1, ..., token7_511]], # åºåˆ—ä½ç½®7
            
            # æ‰¹æ¬¡1  
            [[token8_0, token8_1, ..., token8_511],
            [token9_0, token9_1, ..., token9_511],
            ...,
            [token15_0, token15_1, ..., token15_511]]
        ]
        å˜æ¢å
        hidden_states = hidden_states.view(-1, 512)  # [16, 512]

        # ç°åœ¨çš„é€»è¾‘ç»“æ„ï¼š
        [
            [token0_0, token0_1, ..., token0_511],   # æ‰¹æ¬¡0, ä½ç½®0
            [token1_0, token1_1, ..., token1_511],   # æ‰¹æ¬¡0, ä½ç½®1
            ...,
            [token7_0, token7_1, ..., token7_511],   # æ‰¹æ¬¡0, ä½ç½®7
            [token8_0, token8_1, ..., token8_511],   # æ‰¹æ¬¡1, ä½ç½®0
            [token9_0, token9_1, ..., token9_511],   # æ‰¹æ¬¡1, ä½ç½®1
            ...,
            [token15_0, token15_1, ..., token15_511] # æ‰¹æ¬¡1, ä½ç½®7
        ]

        æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜çš„æ‰¹æ¬¡å’Œåºåˆ—é…ç½®
        def calculate_optimal_config(available_memory):
        # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜çš„æ‰¹æ¬¡å’Œåºåˆ—é…ç½®
        max_tokens = available_memory / bytes_per_token
        
        # åœ¨æ‰¹æ¬¡å’Œåºåˆ—é—´æƒè¡¡
        possible_configs = []
        for seq_len in [512, 1024, 2048, 4096]:
            batch_size = max_tokens // seq_len
            if batch_size >= 1:
                possible_configs.append((batch_size, seq_len))
        
        return possible_configs
        """
      
        logits = F.linear(hidden_states, self.weight, None) 
        #[batch_size*seq_len,n_routed_experts]
        # è®¡ç®—hidden_stateså¯¹åº”çš„ï¼Œæ¯ä¸ªä¸“å®¶çš„åˆ†æ•° hidden_states @ weigth.T
        """ logits 
        æ³¨æ„æƒé‡çš„å½¢çŠ¶ï¼š
        self.weight.shape = [n_routed_experts,h] # [4,512]
        è€Œä¸æ˜¯ä¼ ç»Ÿçš„[h,n_routed_experts]

        è¿™æ ·è®¾è®¡æ˜¯å› ä¸º
        logits = hidden_states @ self.weight.T # [16,512] @ [512,4] = [16,4]
        #æ¯ä¸ªtokenå¾—åˆ°4ä¸ªä¸“å®¶çš„åˆ†æ•°
        
        ä¸æ™®é€šå…¨è¿æ¥å±‚çš„åŒºåˆ«
            æ™®é€šå…¨è¿æ¥å±‚ï¼ˆçº¿æ€§å±‚ï¼‰ï¼š
                å‚è€ƒé—¨æ§å‰é¦ˆç½‘ç»œå±‚å’Œä¼ ç»Ÿå‰é¦ˆç½‘ç»œå±‚
                ç”¨äºç‰¹å¾å˜åŒ–
                self.fc = nn.Linear(512,256)  # 512 -> 256ç»´åº¦
                output = self.fc(hidden_states) # ç‰¹å¾çš„å‡ç»´/é™ç»´
            MoEé—¨æ§çº¿æ€§å±‚
                #ç”¨äºä¸“å®¶é€‰æ‹©ï¼ˆä¸€ä¸ªä¸“å®¶å°±æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼ˆå‰é¦ˆç½‘ç»œå±‚ï¼‰ï¼‰
                logits = F.linear(hidden_states,self.weight,None) # 512 -> 4ç»´
                #ç›®çš„ä¸æ˜¯ç‰¹å¾å˜åŒ–ï¼Œè€Œæ˜¯ä¸ºäº†è®¡ç®—é€‰æ‹©åˆ†æ•°
        è®¡ç®—å¤æ‚åº¦ï¼š
            è®¡ç®—é‡ = bsz * seq_len* h* n_routed_experts
            ç¤ºä¾‹ï¼š16 * 512 * 4 = 32768æ¬¡ä¹˜åŠ è¿ç®—
            
            ç›¸å¯¹äºæ•´ä¸ªæ¨¡å‹æ¥è¯´è®¡ç®—é‡å¾ˆå°
            ä½†æ˜¯å´æ˜¯MoEè·¯ç”±å†³ç­–çš„æ ¸å¿ƒ
        æ€»ç»“ï¼š
        ä¸“å®¶è¯„åˆ†ï¼šä¸ºæ¯ä¸ªtokenè®¡ç®—å¯¹æ‰€æœ‰ä¸“å®¶çš„"åå¥½åˆ†æ•°"
        è·¯ç”±å†³ç­–ï¼šåŸºäºè¿™äº›åˆ†æ•°å†³å®šæ¯ä¸ªtokenä½¿ç”¨å“ªäº›ä¸“å®¶
        å¯å­¦ä¹ è·¯ç”±ï¼šé€šè¿‡è®­ç»ƒä¼˜åŒ–æƒé‡ï¼Œè®©ä¸“å®¶ä¸“ä¸šåŒ–
        é«˜æ•ˆè®¡ç®—ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰tokenï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶å¹¶è¡Œæ€§
        """
        if self.scoring_func == 'softmax':
            # å°†ä¸“å®¶åˆ†æ•°çŸ©é˜µè½¬åŒ–ä¸ºæ¦‚ç‡çŸ©é˜µï¼ˆèµ°ä¸€ä¸‹æ¿€æ´»å‡½æ•°ï¼‰
            scores = logits.softmax(dim=1)
        else:
            raise NotImplementedError(f'ä¸æ”¯æŒè¯„åˆ†å‡½æ•°ï¼Œ{self.scoring_func}')
        
        # ä¸“å®¶é€‰æ‹©  
        """
        torch.topk() :  æ ¹æ®top_kæ¥è®¾ç½®å…·ä½“çš„ä¸ªæ•°
        è¿™ä¸ªå‡½æ•°è¿”å›ä¸€ä¸ªå¼ é‡ä¸­æŒ‡å®šç»´åº¦ä¸Šæœ€å¤§ï¼ˆæœ€å°ï¼‰çš„kä¸ªå€¼ï¼Œ
        ä»¥åŠè¿™äº›å€¼å¯¹åº”çš„ç´¢å¼•ä½ç½®
        """
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)
        # topk_weight: [batch_size*seq_len,top_k] -é€‰ä¸­çš„ä¸“å®¶æƒé‡
        # topk_idk:    [batch_size*seq_len,top_k] -é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
        """å…·ä½“è®¡ç®—è¿‡ç¨‹
        è¾“å…¥scoresç¤ºä¾‹
        å‡è®¾æœ‰4ä¸ªtokenï¼Œ4ä¸ªä¸“å®¶ï¼Œtop_k=2ï¼š
        scores = [
            [0.10, 0.60, 0.25, 0.05],  # ä¸“å®¶1æœ€é«˜(0.6), ä¸“å®¶2æ¬¡é«˜(0.25) # token0å¯¹4ä¸ªä¸“å®¶çš„æ¦‚ç‡
            [0.40, 0.30, 0.20, 0.10],  # ä¸“å®¶0(0.4), ä¸“å®¶1(0.3) # token1å¯¹4ä¸ªä¸“å®¶çš„æ¦‚ç‡  
            [0.05, 0.10, 0.70, 0.15],  # ä¸“å®¶2(0.7), ä¸“å®¶3(0.15)# token2å¯¹4ä¸ªä¸“å®¶çš„æ¦‚ç‡
            [0.25, 0.25, 0.25, 0.25]   # ä»»æ„ä¸¤ä¸ªä¸“å®¶(å„0.25)# token3å¯¹4ä¸ªä¸“å®¶çš„æ¦‚ç‡
        ]
        Top-Kæ“ä½œç»“æœ
        topk_weight, topk_idx = torch.topk(scores, k=2, dim=-1, sorted=False)

        # topk_idx (é€‰æ‹©çš„ä¸“å®¶ç´¢å¼•):
        [
            [1, 2],  # token0: é€‰æ‹©ä¸“å®¶1å’Œä¸“å®¶2
            [0, 1],  # token1: é€‰æ‹©ä¸“å®¶0å’Œä¸“å®¶1
            [2, 3],  # token2: é€‰æ‹©ä¸“å®¶2å’Œä¸“å®¶3
            [0, 1]   # token3: é€‰æ‹©ä¸“å®¶0å’Œä¸“å®¶1 (ä»»æ„ä¸¤ä¸ª)
        ]

        # topk_weight (å¯¹åº”çš„æƒé‡):
        [
            [0.60, 0.25],  # token0: ä¸“å®¶1æƒé‡0.6, ä¸“å®¶2æƒé‡0.25
            [0.40, 0.30],  # token1: ä¸“å®¶0æƒé‡0.4, ä¸“å®¶1æƒé‡0.3
            [0.70, 0.15],  # token2: ä¸“å®¶2æƒé‡0.7, ä¸“å®¶3æƒé‡0.15
            [0.25, 0.25]   # token3: ä¸¤ä¸ªä¸“å®¶å„0.25
        ]
        """

        # å½’ä¸€åŒ–top-kæ¦‚ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1,keepdim=True) + 1e-20 #é˜²æ­¢é™¤é›¶
            topk_weight = topk_weight/denominator #å½’ä¸€åŒ–ï¼Œä½¿å¾—é€‰ä¸­çš„kä¸ªä¸“å®¶æƒé‡å’Œä¸º1

        # è®­ç»ƒæ—¶è®¡ç®—è¾…åŠ©æŸå¤±ï¼Œç¡®ä¿ä¸“å®¶è´Ÿè½½å‡è¡¡
        if self.training and self.aplha > 0.0:
            scores_for_aux = scores
            aux_topx = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz,-1)
            """å½¢çŠ¶å˜æ¢è¯¦è§£
            å˜æ¢å‰: 
            topk_idx.shape = [bsz*seq_len, top_k]  # [16, 2]
            # 16ä¸ªtokenï¼Œæ¯ä¸ªtokené€‰æ‹©2ä¸ªä¸“å®¶
            å˜æ¢å
            topk_idx_for_aux_loss.shape = [bsz, seq_len * top_k]  # [2, 16]
            # 2ä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡æœ‰16ä¸ªä¸“å®¶é€‰æ‹©ï¼ˆ8ä¸ªtoken Ã— æ¯ä¸ªé€‰2ä¸ªä¸“å®¶ï¼‰
            å…·ä½“æ•°å€¼ç¤ºä¾‹
            å‡è®¾ï¼š
            bsz = 2 (2ä¸ªåºåˆ—)
            seq_len = 8 (æ¯ä¸ªåºåˆ—8ä¸ªtoken)
            top_k = 2 (æ¯ä¸ªtokené€‰2ä¸ªä¸“å®¶)

            å˜æ¢å‰æ•°æ®:
                # topk_idx: [16, 2] - 16ä¸ªtokençš„ä¸“å®¶é€‰æ‹©
                topk_idx = [
                    # æ‰¹æ¬¡0çš„8ä¸ªtoken
                    [1, 2], [0, 1], [2, 3], [0, 2], [1, 3], [0, 3], [2, 1], [3, 0],
                    # æ‰¹æ¬¡1çš„8ä¸ªtoken  
                    [0, 2], [1, 3], [2, 0], [3, 1], [0, 1], [2, 3], [1, 0], [3, 2]
                ]
            å˜æ¢åæ•°æ®
                topk_idx_for_aux_loss = topk_idx.view(2, -1)
                # å½¢çŠ¶: [2, 16]

                topk_idx_for_aux_loss = [
                    # æ‰¹æ¬¡0: æ‰€æœ‰16ä¸ªä¸“å®¶é€‰æ‹©ï¼ˆ8tokenÃ—2ä¸“å®¶ï¼‰
                    [1, 2, 0, 1, 2, 3, 0, 2, 1, 3, 0, 3, 2, 1, 3, 0],
                    
                    # æ‰¹æ¬¡1: æ‰€æœ‰16ä¸ªä¸“å®¶é€‰æ‹©
                    [0, 2, 1, 3, 2, 0, 3, 1, 0, 1, 2, 3, 1, 0, 3, 2]
                ]
            """

            if self.seq_aux:
                #åºåˆ—çº§è¾…åŠ©æŸå¤±
                aux_loss = self._compute_sequence_aux_loss(
                    scores_for_aux,
                    topk_idx_for_aux_loss,
                    bsz,
                    seq_len,
                    aux_topx,
                    hidden_states.device
                )
            else:
                # Tokençº§è¾…åŠ©æŸå¤±
                aux_loss = self._compute_token_aux_loss(
                    scores_for_aux, 
                    topk_idx_for_aux_loss,
                    bsz, 
                    seq_len,
                    aux_topx,
                    hidden_states.device
                )
        else:
            aux_topx = 0
        return topk_idx, topk_weight, aux_loss

    # è®¡ç®—åºåˆ—çº§è¾…åŠ©æŸå¤±
    def _compute_sequence_aux_loss(
            self, 
            scores_for_aux,
            topk_idx_for_aux_loss, 
            bsz, 
            seq_len, 
            aux_topk, 
            device
    ):
       
        scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)
        """å½¢çŠ¶å˜åŒ–è¯¦è§£ï¼š
        å˜åŒ–å‰ï¼š
            scores_for_aux.shape = [bsz*seq_len,n_routed_experts] # [16*4]
            # å±•å¹³åçš„æ‰€æœ‰tokenå¯¹ä¸“å®¶çš„åŸå§‹åˆ†æ•°
        å˜åŒ–åï¼š
            scores_for_seq_aux.shape = [bsz,seq_len,n_routed_experts] # [2,8,4]
            #æ¢å¤æ‰¹æ¬¡å’Œåºåˆ—ç»´åº¦ï¼Œä¿æŒä¸“å®¶ç»´åº¦ 
        å…·ä½“æ•°å€¼ç¤ºä¾‹
        å‡è®¾ï¼š
        bsz = 2 (2ä¸ªåºåˆ—)
        seq_len = 8 (æ¯ä¸ªåºåˆ—8ä¸ªtoken)
        n_routed_experts = 4 (4ä¸ªä¸“å®¶)
        å˜æ¢å‰æ•°æ®
        # scores_for_aux: [16, 4] - 16ä¸ªtokenå¯¹4ä¸ªä¸“å®¶çš„åŸå§‹åˆ†æ•°
        scores_for_aux = [
            # æ‰¹æ¬¡0çš„8ä¸ªtoken
            [0.10, 0.60, 0.25, 0.05],  # token0
            [0.40, 0.30, 0.20, 0.10],  # token1
            [0.05, 0.10, 0.70, 0.15],  # token2
            [0.25, 0.25, 0.25, 0.25],  # token3
            [0.15, 0.50, 0.20, 0.15],  # token4
            [0.30, 0.20, 0.40, 0.10],  # token5
            [0.20, 0.30, 0.10, 0.40],  # token6
            [0.10, 0.20, 0.30, 0.40],  # token7
            
            # æ‰¹æ¬¡1çš„8ä¸ªtoken
            [0.50, 0.20, 0.20, 0.10],  # token8
            [0.25, 0.25, 0.25, 0.25],  # token9
            [0.10, 0.60, 0.20, 0.10],  # token10
            [0.30, 0.30, 0.20, 0.20],  # token11
            [0.15, 0.25, 0.35, 0.25],  # token12
            [0.40, 0.30, 0.15, 0.15],  # token13
            [0.20, 0.40, 0.25, 0.15],  # token14
            [0.10, 0.15, 0.35, 0.40]   # token15
        ]
        å˜æ¢åæ•°æ®
        scores_for_seq_aux = scores_for_aux.view(2, 8, 4)
        # å½¢çŠ¶: [2, 8, 4]

        scores_for_seq_aux = [
            # æ‰¹æ¬¡0
            [
                [0.10, 0.60, 0.25, 0.05],  # ä½ç½®0
                [0.40, 0.30, 0.20, 0.10],  # ä½ç½®1
                [0.05, 0.10, 0.70, 0.15],  # ä½ç½®2
                [0.25, 0.25, 0.25, 0.25],  # ä½ç½®3
                [0.15, 0.50, 0.20, 0.15],  # ä½ç½®4
                [0.30, 0.20, 0.40, 0.10],  # ä½ç½®5
                [0.20, 0.30, 0.10, 0.40],  # ä½ç½®6
                [0.10, 0.20, 0.30, 0.40]   # ä½ç½®7
            ],
            
            # æ‰¹æ¬¡1
            [
                [0.50, 0.20, 0.20, 0.10],  # ä½ç½®0
                [0.25, 0.25, 0.25, 0.25],  # ä½ç½®1
                [0.10, 0.60, 0.20, 0.10],  # ä½ç½®2
                [0.30, 0.30, 0.20, 0.20],  # ä½ç½®3
                [0.15, 0.25, 0.35, 0.25],  # ä½ç½®4
                [0.40, 0.30, 0.15, 0.15],  # ä½ç½®5
                [0.20, 0.40, 0.25, 0.15],  # ä½ç½®6
                [0.10, 0.15, 0.35, 0.40]   # ä½ç½®7
            ]
        ]

        ï¼ˆå›é¡¾ä¸‹topK_idx_for_aux_lossï¼‰
        topk_idx_for_aux_loss = [
            # æ‰¹æ¬¡0: æ‰€æœ‰16ä¸ªä¸“å®¶é€‰æ‹©ï¼ˆ8tokenÃ—2ä¸“å®¶ï¼‰
            [1, 2, 0, 1, 2, 3, 0, 2, 1, 3, 0, 3, 2, 1, 3, 0],
            
            # æ‰¹æ¬¡1: æ‰€æœ‰16ä¸ªä¸“å®¶é€‰æ‹©
            [0, 2, 1, 3, 2, 0, 3, 1, 0, 1, 2, 3, 1, 0, 3, 2]
        ]
        """

        # è®¡ç®—æ¯ä¸ªbatchä¸­æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡
        ce = torch.zeros(bsz, self.n_routed_experts,device=device)
        """
        target.scatter_add_(dim, index, source)
        åœ¨dimç»´åº¦ä¸Šï¼Œå°†sourceçš„å€¼æŒ‰ç…§indexæŒ‡å®šçš„ä½ç½®ç´¯åŠ åˆ°targetä¸­
        å…·ä½“è®¡ç®—è§„åˆ™
        å¯¹äºæ¯ä¸ªä½ç½®(i, j)ï¼š
        target[i][index[i][j]] += source[i][j]
        æœ€ç»ˆç»“æœï¼š
        ce = [
            [3, 3, 2],  # æ‰¹æ¬¡0: ä¸“å®¶0è¢«é€‰3æ¬¡ï¼Œä¸“å®¶1è¢«é€‰3æ¬¡ï¼Œä¸“å®¶2è¢«é€‰2æ¬¡
            [2, 4, 2]   # æ‰¹æ¬¡1: ä¸“å®¶0è¢«é€‰2æ¬¡ï¼Œä¸“å®¶1è¢«é€‰4æ¬¡ï¼Œä¸“å®¶2è¢«é€‰2æ¬¡
        ]
        ç‰©ç†æ„ä¹‰ï¼šä¸“å®¶é€‰æ‹©ç»Ÿè®¡
        è´Ÿè½½åˆ†å¸ƒåˆ†æ
        # ç†æƒ³æƒ…å†µï¼ˆå®Œå…¨å‡è¡¡ï¼‰ï¼š
        æœŸæœ›é€‰æ‹©æ¬¡æ•° = (seq_len * aux_topk) / n_routed_experts
                    = (4 * 2) / 3 â‰ˆ 2.67æ¬¡/ä¸“å®¶

        # å®é™…ç»“æœï¼š
        æ‰¹æ¬¡0: [3, 3, 2] â†’ ç›¸å¯¹å‡è¡¡
        æ‰¹æ¬¡1: [2, 4, 2] â†’ ä¸“å®¶1è¿‡è½½ï¼Œä¸“å®¶0å’Œ2æ¬ è½½
        """
        ce.scatter_add_(
            1,
            topk_idx_for_aux_loss, #ç´¢å¼•
            torch.ones(bsz, seq_len*aux_topk, device=device) #å€¼
        )
        ce.div_(seq_len*aux_topk/self.n_routed_experts) #å½’ä¸€åŒ–

        # è®¡ç®—æŸå¤±ï¼šä½¿ç”¨é¢‘ç‡*å¹³å‡åˆ†æ•°
        aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean()*self.aplha
        return aux_loss
   
    # Tokençº§è¾…åŠ©æŸå¤±å®ç°
    def _compute_token_aux_loss(self, scores_for_aux, topk_idx_for_aux_loss, bsz, seq_len, aux_topk, device):
        # åˆ›å»ºone-hotç¼–ç çš„æ©ç 
        mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1),num_classes=self.n_routed_experts)

        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å®é™…ä½¿ç”¨é¢‘ç‡
        ce = mask_ce.float().mean()

        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡é€‰æ‹©é¢‘ç‡
        Pi = scores_for_aux.mean(0)

        # è®¡ç®—ç†æƒ³ä½¿ç”¨é¢‘ç‡çš„åå·®
        fi = ce*self.n_routed_experts #ç¼©æ”¾ä»¥åŒ¹é…æœŸæœ›å€¼

        # è®¡ç®—è¾…åŠ©æŸå¤±

        aux_loss = (Pi * fi).sum() * self.aplha

        return aux_loss

class MOEFeedForward(nn.Module):
    def __init__(self, config:MiniMindConfig):
        super().__init__()
        self.config = config
        # ä¸“å®¶ç½‘ç»œåˆ—è¡¨
        # nn.ModuleList æ˜¯PyTorchä¸­ç”¨äºå­˜å‚¨å­æ¨¡å—çš„ç‰¹æ®Šå®¹å™¨
        # ä¸æ™®é€šPythonåˆ—è¡¨ä¸åŒï¼Œå®ƒèƒ½ç¡®ä¿å…¶ä¸­çš„æ¨¡å—è¢«æ­£ç¡®æ³¨å†Œåˆ°æ¨¡å‹ä¸­
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])
        # é—¨æ§ç½‘ç»œï¼ˆè®¡ç®—å‡ºæ¥çš„éœ€è¦å‚ä¸è®¡ç®—çš„ä¸“å®¶ï¼‰
        self.gate = MoEGate(config)
        # å…±äº«ä¸“å®¶ï¼ˆå¯é€‰ï¼‰
        if config.n_shared_experts > 0:
            self.share_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])
    
    # è®­ç»ƒå’Œæ¨ç†çš„åŒè·¯å¾„è®¾è®¡
    def forward(self,x):
        identity = x # ä¿å­˜åŸå§‹è¾“å…¥ï¼Œç”¨äºæ®‹å·®é“¾æ¥ï¼ˆæ’ç­‰æ˜ å°„ï¼‰
        orig_shape = x.shape #åŸå§‹è¾“å…¥çš„å½¢çŠ¶
        bsz,seq_len,_ = x.shape #è§£æ„åŸå§‹è¾“å…¥ï¼Œæ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦

        # 1ã€é—¨æ§é€‰æ‹©ä¸“å®¶
        topk_idx,topk_weight,aux_loss = self.gate(x)

        # 2ã€å±•å¹³è¾“å…¥ä»¥ä¾¿å¤„ç†
        x = x.view(-1,x.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        """ å±•å¹³è¯¦è§£
        è¾“å…¥å¼ é‡xå±•å¹³ï¼š
            å˜åŒ–å‰:
                x.shape = [bsz, seq_len, hidden_size] # [2,8,512]
                #2ä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡8ä¸ªtokenï¼Œæ¯ä¸ªtoken 512ç»´
            å˜åŒ–å:
                x = x.view(-1,x.shape[-1])  # [16,512]
                #16ä¸ªtokenï¼Œæ¯ä¸ªtoken 512ç»´
                #-1 è¡¨ç¤ºè‡ªåŠ¨æç«¯ï¼š 2*8 = 16
        ä¸“å®¶ç´¢å¼•å±•å¹³
            å˜åŒ–å‰ï¼š
                topk_idx.shape = [bsz*seq_len,top_k] # [16,2]
                #16ä¸ªtokenï¼Œæ¯ä¸ªtokené€‰æ‹©2ä¸ªä¸“å®¶
            å˜åŒ–åï¼š
                flat_topk_idx = topk_idx.view(-1)  #[32]
                #32ä¸ªä¸“å®¶é€‰æ‹©ï¼ˆ16ä¸ªtoken  x  æ¯ä¸ªé€‰2ä¸ªä¸“å®¶ï¼‰
                #-1è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼š16*2 = 32
        å…·ä½“æ•°å€¼ç¤ºä¾‹ï¼š
        å‡è®¾ï¼š
        bsz = 2, seq_len = 4, hidden_size = 512, top_k = 2
        è¾“å…¥æ•°æ®
        # x (åŸå§‹è¾“å…¥)
        x = [
            # æ‰¹æ¬¡0
            [[t0_0, t0_1, ..., t0_511],  # token0
            [t1_0, t1_1, ..., t1_511],  # token1
            [t2_0, t2_1, ..., t2_511],  # token2
            [t3_0, t3_1, ..., t3_511]], # token3
            
            # æ‰¹æ¬¡1
            [[t4_0, t4_1, ..., t4_511],  # token4
            [t5_0, t5_1, ..., t5_511],  # token5
            [t6_0, t6_1, ..., t6_511],  # token6
            [t7_0, t7_1, ..., t7_511]]  # token7
        ]

        # topk_idx (ä¸“å®¶é€‰æ‹©)
        topk_idx = [
            # æ‰¹æ¬¡0çš„tokenä¸“å®¶é€‰æ‹©
            [1, 2],  # token0: ä¸“å®¶1, ä¸“å®¶2
            [0, 1],  # token1: ä¸“å®¶0, ä¸“å®¶1
            [2, 3],  # token2: ä¸“å®¶2, ä¸“å®¶3
            [0, 2],  # token3: ä¸“å®¶0, ä¸“å®¶2
            
            # æ‰¹æ¬¡1çš„tokenä¸“å®¶é€‰æ‹©  
            [1, 3],  # token4: ä¸“å®¶1, ä¸“å®¶3
            [0, 2],  # token5: ä¸“å®¶0, ä¸“å®¶2
            [1, 0],  # token6: ä¸“å®¶1, ä¸“å®¶0
            [3, 2]   # token7: ä¸“å®¶3, ä¸“å®¶2
        ]

        å˜æ¢åæ•°æ®
        # xå±•å¹³å
        x_flat = [
            [t0_0, t0_1, ..., t0_511],  # token0
            [t1_0, t1_1, ..., t1_511],  # token1
            [t2_0, t2_1, ..., t2_511],  # token2
            [t3_0, t3_1, ..., t3_511],  # token3
            [t4_0, t4_1, ..., t4_511],  # token4
            [t5_0, t5_1, ..., t5_511],  # token5
            [t6_0, t6_1, ..., t6_511],  # token6
            [t7_0, t7_1, ..., t7_511]   # token7
        ]  # [8, 512]

        # flat_topk_idxå±•å¹³å
        flat_topk_idx = [
            1, 2,  # token0çš„2ä¸ªä¸“å®¶
            0, 1,  # token1çš„2ä¸ªä¸“å®¶  
            2, 3,  # token2çš„2ä¸ªä¸“å®¶
            0, 2,  # token3çš„2ä¸ªä¸“å®¶
            1, 3,  # token4çš„2ä¸ªä¸“å®¶
            0, 2,  # token5çš„2ä¸ªä¸“å®¶
            1, 0,  # token6çš„2ä¸ªä¸“å®¶
            3, 2   # token7çš„2ä¸ªä¸“å®¶
        ]  # [16] (8ä¸ªtoken Ã— 2ä¸ªä¸“å®¶ = 16ä¸ªé€‰æ‹©)
        """

        # è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨ä¸åŒçš„è·¯å¾„
        if self.training:
            y = self._moe_train_forward(
                x,
                topk_idx,
                topk_weight,
                flat_topk_idx,
                orig_shape
            )
        else:
            y = self.moe_infer(
                x,
                flat_topk_idx,
                topk_weight.view(-1,1).view(*orig_shape)
            )
        
        # 4ã€æ·»åŠ å…±äº«ä¸“å®¶è¾“å‡º
        if self.config.n_shared_experts > 0:
            for expert in self.share_experts:
                y = y + expert(identity)
        
        self.aux_loss = aux_loss

        return y

    def _moe_train_forward(self, x, topk_idx, topk_weight, flat_topk_idx, orig_shape):
        """è®­ç»ƒæ¨¡å¼ä¸‹çš„å‰å‘ä¼ æ’­"""
        # å°†æ¯ä¸ªtokené‡å¤top_kæ¬¡ï¼Œä»¥ä¾¿å¹¶è¡Œå¤„ç†æ‰€æœ‰é€‰ä¸­çš„ä¸“å®¶ï¼ˆï¼ï¼ï¼å› ä¸ºæ¯ä¸€ä¸ªtokenéƒ½æ˜¯è¦top_kä¸ªä¸“å®¶æ¥è¿›è¡Œå¤„ç†çš„ï¼‰
        # num_experts_per_tok æ¯ä¸ªtokenç”±å‡ ä¸ªä¸“å®¶å¤„ç†
        x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
        """x_flat_repeat = [
                [t0_0, t0_1, ..., t0_511],  # token0
                [t0_0, t0_1, ..., t0_511],  # token0
                [t1_0, t1_1, ..., t1_511],  # token1
                [t1_0, t1_1, ..., t1_511],  # token1
                [t2_0, t2_1, ..., t2_511],  # token2
                [t2_0, t2_1, ..., t2_511],  # token2
                [t3_0, t3_1, ..., t3_511],  # token3
                [t3_0, t3_1, ..., t3_511],  # token3
                [t4_0, t4_1, ..., t4_511],  # token4
                [t4_0, t4_1, ..., t4_511],  # token4
                [t5_0, t5_1, ..., t5_511],  # token5
                [t5_0, t5_1, ..., t5_511],  # token5
                [t6_0, t6_1, ..., t6_511],  # token6
                [t6_0, t6_1, ..., t6_511],  # token6
                [t7_0, t7_1, ..., t7_511]   # token7
                [t7_0, t7_1, ..., t7_511]   # token7
            ]  # [16, 512]
            ä¸flat_topk_idxä¸€ ä¸€å¯¹åº”
            flat_topk_idx = [
                1, 2,  # token0çš„2ä¸ªä¸“å®¶
                0, 1,  # token1çš„2ä¸ªä¸“å®¶  
                2, 3,  # token2çš„2ä¸ªä¸“å®¶
                0, 2,  # token3çš„2ä¸ªä¸“å®¶
                1, 3,  # token4çš„2ä¸ªä¸“å®¶
                0, 2,  # token5çš„2ä¸ªä¸“å®¶
                1, 0,  # token6çš„2ä¸ªä¸“å®¶
                3, 2   # token7çš„2ä¸ªä¸“å®¶
            ]  # [16] (8ä¸ªtoken Ã— 2ä¸ªä¸“å®¶ = 16ä¸ªé€‰æ‹©)
        """
        # åˆå§‹åŒ–è¾“å‡ºï¼ˆè¿”å›ä¸€ä¸ªä¸è¾“å…¥å¼ é‡xå½¢çŠ¶ç›¸åŒï¼Œä½†æ˜¯æ•°æ®ç±»å‹ä¸ºfloat16çš„æœªåˆå§‹åŒ–çš„å¼ é‡ï¼‰
        y=torch.empty_like(x,dtype=torch.float16)  #ä½¿ç”¨fp16èŠ‚çœå†…å­˜

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰ä¸“å®¶
        for i,   in enumerate(self.experts):
            # æ‰¾å‡ºåº”è¯¥ç”±å½“å‰ä¸“å®¶å¤„ç†çš„token   
            """
            æ¯”å¦‚i = 0,maskä¼šè¿”å›ä¸€ä¸ªé•¿åº¦ä¸flat_topk_idxç›¸ç­‰çš„å¸ƒå°”æ©ç çŸ©é˜µ
            mask = (flat_topk_idx == 1)
            # ç»“æœ: [True, False, False, True, False, False, False, False, True, False, False, False, True, False, False, False]
            """
            mask = flat_topk_idx == i
            """ æ³¨é‡Š
            mask.any(): # è‡³å°‘æœ‰ä¸€ä¸ªTrue
            # æå–ä¸“å®¶içš„è¾“å…¥
            x[mask]:æ˜ å°„æ‰¾å‡ºç›¸åº”çš„xï¼ˆtokenï¼‰ï¼Œ
            # ä¸“å®¶è®¡ç®—
            expert(x[mask]): é€å…¥è¿™ä¸ªä¸“å®¶å»è¿›è¡Œè®¡ç®—ï¼ˆå…¶å®ä¹Ÿå°±æ˜¯è¿›è¡Œäº†ä¸€æ¬¡æ™®é€šçš„é—¨æ§å‰é¦ˆï¼Œç„¶åè¾“å‡ºï¼‰
                1. ç¨€ç–å¤„ç†
                    # åªå¯¹éœ€è¦å¤„ç†çš„tokenè¿›è¡Œè®¡ç®—ï¼Œè·³è¿‡å…¶ä»–token
                    # é¿å…äº†if-elseåˆ¤æ–­ï¼Œåˆ©ç”¨å‘é‡åŒ–æ“ä½œ
                2. å†…å­˜å±€éƒ¨æ€§
                    # maskåˆ›å»ºåï¼Œx[mask]ä¼šæå–è¿ç»­çš„å†…å­˜å—
                    # ä¸“å®¶è®¡ç®—æ—¶è·å¾—è¿ç»­çš„è¾“å…¥ï¼Œæé«˜ç¼“å­˜æ•ˆç‡
                3. è‡ªåŠ¨æ‰¹é‡å¤„ç†
                    # å³ä½¿ä¸€ä¸ªä¸“å®¶å¤„ç†å¤šä¸ªä¸è¿ç»­çš„tokenå‰¯æœ¬
                    # x[mask]ä¼šè‡ªåŠ¨å°†å®ƒä»¬æ”¶é›†ä¸ºè¿ç»­æ‰¹æ¬¡
                    # ä¸“å®¶ä»ç„¶å¯ä»¥ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰åˆ†é…çš„ä»»åŠ¡
            æ¢¯åº¦æµåŠ¨
                # maskä¹Ÿç”¨äºç¡®ä¿æ¢¯åº¦æ­£ç¡®å›ä¼ ï¼š
                # - åªæœ‰è¢«å¤„ç†çš„tokenå‰¯æœ¬çš„æ¢¯åº¦ä¼šæ›´æ–°ä¸“å®¶içš„å‚æ•°
                # - å…¶ä»–ä¸“å®¶çš„æ¢¯åº¦ä¸å—å½±å“
            è®¡ç®—å›¾è¿æ¥
                # é€šè¿‡maskç´¢å¼•å»ºç«‹çš„è¿æ¥æ˜¯å¯å¾®åˆ†çš„
                # æ¢¯åº¦å¯ä»¥ä»y[mask]æµå›expert_outputï¼Œå†æµå›ä¸“å®¶ç½‘ç»œ
            # å°†ç»“æœå­˜å›è¾“å‡ºå¼ é‡
            y[mask]=expert_output.to(y.dtype)
            """
            if mask.any():
                # ä¸“å®¶å¤„ç†å¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
                expert_output = expert(x[mask])
                y[mask] = expert_output.to(y.dtype)
        # åŠ æƒæ±‚å’Œï¼šå°†å¤šä¸ªä¸“å®¶çš„è¾“å‡ºæŒ‰æƒé‡åˆå¹¶
        # yçš„å½¢çŠ¶ï¼š[bsz*seq_len*top_k, hidden_size]-> [bsz*seq_len,top_k,hidden_size] -> [bsz*seq_len,hidden_size]
        y = (y.view(*topk_weight.shape, -1)*topk_weight.unsqueeze(-1)).sum(dim=1)

        # æ¢å¤åŸå§‹å½¢çŠ¶
        y = y.view(*orig_shape)
        return y
    
    @torch.no_grad()
    def moe_infer(self,x,flat_expert_idices, flat_expert_weights):
        """æ¨ç†ä¼˜åŒ–çš„MoEå‰å‘ä¼ æ’­"""
        # åˆå§‹åŒ–è¾“å‡ºç¼“å­˜
        expert_cache=torch.zeros_like(x)

        # æŒ‰ä¸“å®¶ç´¢å¼•æ’åºï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†åŒä¸€ä¸“å®¶çš„token
        """argsort()æ–¹æ³•çš„ä½œç”¨
        argsort()è¿”å›çš„æ˜¯æ’åºåçš„ç´¢å¼•ï¼Œè€Œä¸æ˜¯æ’åºåçš„å€¼
        å³ï¼šè¿”å›ä¸€ä¸ªç´¢å¼•çš„æ•°ç»„ï¼Œä½¿å¾—flat_expert_indices[idxs]æ˜¯å‡åºæ’åˆ—çš„

        å…·ä½“è®¡ç®—è¿‡ç¨‹
        æ’åºå‰çš„æ•°æ®
            # åŸå§‹ ä¸“å®¶ç´¢å¼•å’Œä½ç½®
            ä½ç½®:   0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15
            å€¼:    [1, 2, 0, 1, 2, 3, 0, 2, 1, 3, 0, 2, 1, 0, 3, 2]
        argsort()æ“ä½œ
            #å¯¹å€¼è¿›è¡Œå‡åºæ’åºï¼Œä½†æ˜¯è¿”å›çš„æ˜¯å€¼çš„ç´¢å¼•
            idxs = flat_expert_indices.argsort()
            # ç»“æœ: [2, 6, 10, 13, 0, 3, 8, 12, 1, 4, 7, 11, 15, 5, 9, 14]
        
        æ’åºåï¼Œç›¸åŒä¸“å®¶çš„tokenå‰¯æœ¬è¢«èšé›†åœ¨ä¸€èµ·
        åˆ†ç»„ç»“æœ = [
            # ä¸“å®¶0çš„tokenå‰¯æœ¬: ä½ç½®2,6,10,13 (å‰4ä¸ª)
            # ä¸“å®¶1çš„tokenå‰¯æœ¬: ä½ç½®0,3,8,12 (æ¥ä¸‹æ¥4ä¸ª)  
            # ä¸“å®¶2çš„tokenå‰¯æœ¬: ä½ç½®1,4,7,11,15 (æ¥ä¸‹æ¥5ä¸ª)
            # ä¸“å®¶3çš„tokenå‰¯æœ¬: ä½ç½®5,9,14 (æœ€å3ä¸ª)
        ]

        # æ’åºåï¼Œå¯ä»¥æŒ‰ä¸“å®¶é¡ºåºæ‰¹é‡å¤„ç†ï¼š
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        # ç»“æœ: [4, 8, 13, 16]  # æ¯ä¸ªä¸“å®¶çš„ç»“æŸä½ç½®

        # å¤„ç†ä¸“å®¶0: idxs[0:4]   â†’ ä½ç½®2,6,10,13
        # å¤„ç†ä¸“å®¶1: idxs[4:8]   â†’ ä½ç½®0,3,8,12  
        # å¤„ç†ä¸“å®¶2: idxs[8:13]  â†’ ä½ç½®1,4,7,11,15
        # å¤„ç†ä¸“å®¶3: idxs[13:16] â†’ ä½ç½®5,9,14

        å®Œæ•´æ¨ç†æµç¨‹æ•°æ®
        # å‡è®¾æ•°æ®ï¼š
        flat_expert_indices =
            [ 1, 2,  0,  1,  2,  3,  0,  2,  1,  3,   0,   2,   1,    0,   3,  2 ]
        x = [t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15]  # 16ä¸ªtokenå‰¯æœ¬

        # argsort() åï¼š
        idxs = [ 2, 6, 10, 13,    0, 3, 8, 12,     1, 4, 7, 11, 15,      5, 9, 14]

        # å¯¹åº”çš„tokené¡ºåºï¼ˆæŒ‰ä¸“å®¶åˆ†ç»„ï¼‰ï¼š
        ä¸“å®¶0: [t2, t6, t10, t13]    # æ¥è‡ªä½ç½®2,6,10,13
        ä¸“å®¶1: [t0, t3, t8, t12]     # æ¥è‡ªä½ç½®0,3,8,12  
        ä¸“å®¶2: [t1, t4, t7, t11, t15] # æ¥è‡ªä½ç½®1,4,7,11,15
        ä¸“å®¶3: [t5, t9, t14]         # æ¥è‡ªä½ç½®5,9,14
        """
        idxs = flat_expert_idices.argsort()

        # è®¡ç®—æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡
        """
        flat_expert_idices ä¸€ç»´çš„å¼ é‡
        bincount():  torchçš„å¼ é‡æ–¹æ³•ï¼Œç”¨äºè®¡ç®—æ¯ä¸ªæ•´æ•°å€¼å‡ºç°çš„æ¬¡æ•°ï¼Œè¿”å›ä¸€ä¸ªå¼ é‡ï¼Œé•¿åº¦æ˜¯flat_expert_idicesçš„ä¸­çš„æœ€å¤§å€¼åŠ 1
        .cup(): å°†å¼ é‡ä»å½“å‰è®¾å¤‡ï¼ˆå¦‚gpuï¼‰ï¼Œç§»åŠ¨åˆ°cpuï¼Œå¦‚æœå¼ é‡å·²ç»åœ¨cpuä¸Šï¼Œåˆ™è¿™ä¸ªæ“ä½œä¸ä¼šæ”¹å˜ä»€ä¹ˆ
        numpy(): å°†pytorchå¼ é‡è½¬åŒ–æœªnumpyæ•°ç»„
        cumsum(0):å¯¹Numpyæ•°ç»„è¿›è¡Œç´¯åŠ æ±‚å’Œï¼Œæ²¿ç€ç¬¬ä¸€ä¸ªè½´ï¼ˆ0è½´ï¼Œå¯¹äºä¸€ç»´æ•°ç»„å°±æ˜¯æ²¿ç€æ•°ç»„é¡ºåºç´¯åŠ ï¼‰
        """
        tokens_per_expert = flat_expert_idices.bincount().cpu.numpy().cumsum(0)

        # è·å–tokenç´¢å¼•ï¼Œï¼ˆå»é™¤ä¸“å®¶é‡å¤ç»´åº¦ï¼‰
        """
        ä»”ç»†çœ‹è§„å¾‹ï¼š
        åœ¨ä¸Šé¢çš„è®­ç»ƒæ¨¡å¼ä¸­ï¼Œæ˜ç¡®åœ°å¤åˆ¶æ¯ä¸ªtokenï¼š
        x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
        å‡è®¾æœ‰4ä¸ªåŸå§‹tokenï¼Œtop_k=2ï¼š
        åŸå§‹token: [t0, t1, t2, t3]  # 4ä¸ªtoken
        å¤åˆ¶å: [t0, t0, t1, t1, t2, t2, t3, t3]  # 8ä¸ªå‰¯æœ¬
        ç´¢å¼•:     0,  1,  2,  3,  4,  5,  6,  7

        2. å¤åˆ¶æ¨¡å¼çš„æ•°å­¦è§„å¾‹
        è§‚å¯Ÿå¤åˆ¶åçš„ç´¢å¼•æ¨¡å¼ï¼š
            åŸå§‹token0 â†’ å‰¯æœ¬0,1
            åŸå§‹token1 â†’ å‰¯æœ¬2,3  
            åŸå§‹token2 â†’ å‰¯æœ¬4,5
            åŸå§‹token3 â†’ å‰¯æœ¬6,7
        å‘ç°è§„å¾‹ï¼š
        åŸå§‹tokenç´¢å¼• = å‰¯æœ¬ç´¢å¼• // 2

        éªŒè¯ï¼š
        å‰¯æœ¬0: 0//2=0 â†’ token0
        å‰¯æœ¬1: 1//2=0 â†’ token0  
        å‰¯æœ¬2: 2//2=1 â†’ token1
        å‰¯æœ¬3: 3//2=1 â†’ token1
        å‰¯æœ¬4: 4//2=2 â†’ token2
        å‰¯æœ¬5: 5//2=2 â†’ token2
        å‰¯æœ¬6: 6//2=3 â†’ token3
        å‰¯æœ¬7: 7//2=3 â†’ token3

        å¦‚æœæˆ‘ä»¬æŠŠæ¨ç†ä¼˜åŒ–çš„æ–¹æ¡ˆç”¨äºè®­ç»ƒå°±æ˜¯å¦‚ä¸‹æ•ˆæœï¼š
        å‡è®¾æˆ‘ä»¬æœ‰8ä¸ªåŸå§‹tokenï¼Œæ¯ä¸ªé€‰2ä¸ªä¸“å®¶ï¼š
        åŸå§‹token: [t0, t1, t2, t3, t4, t5, t6, t7]
        å¤åˆ¶å: [t0,t0, t1,t1, t2,t2, t3,t3, t4,t4, t5,t5, t6,t6, t7,t7]
        å‰¯æœ¬ç´¢å¼•: 0,1,  2,3,  4,5,  6,7,  8,9,  10,11, 12,13, 14,15
        ä¸“å®¶é€‰æ‹©åçš„æ··ä¹±å±€é¢
        ç»è¿‡é—¨æ§å’Œæ’åºåï¼Œå‰¯æœ¬é¡ºåºè¢«æ‰“ä¹±ï¼š
        æ’åºåå‰¯æœ¬ç´¢å¼•: [2,6,10,13,0,3,8,12,1,4,7,11,15,5,9,14]
        å¯¹åº”çš„ä¸“å®¶:     [0,0, 0, 0, 1,1,1, 1, 2,2,2,2, 2, 3,3,3]

        æ˜ å°„å›åŸå§‹token
        å‰¯æœ¬ç´¢å¼•: 2,6,10,13,0,3,8,12,1,4,7,11,15,5,9,14
        //2æ“ä½œ: 1,3, 5, 6,0,1,4, 6,0,2,3,5, 7,2,4,7

        å¾—åˆ°åŸå§‹token: [t1,t3,t5,t6, t0,t1,t4,t6, t0,t2,t3,t5,t7,t2,t4,t7]


        ç„¶åæˆ‘ä»¬çœ‹æ¨ç†ä¼˜åŒ–
        1. æ•°æ®æ²¡æœ‰å®é™…å¤åˆ¶
        2. é€šè¿‡ç´¢å¼•æ¨¡æ‹Ÿå¤åˆ¶
        # ä¸å¤åˆ¶æ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡ç´¢å¼•æ¥è¾¾åˆ°ç›¸åŒæ•ˆæœ
        expert_input = x[original_indices]  # ç›´æ¥è®¿é—®åŸå§‹æ•°æ®
        x_copied[copy_idx] = x[copy_idx // top_k]
        """
        token_idxs = idxs // self.config.num_experts_per_tok

        # æ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶çš„token
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i-1]
            if start_idx == end_idx: # è¯¥ä¸“å®¶æ²¡æœ‰tokenè¦å¤„ç†
                continue

            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx: end_idx]
            expert_tokens = x[exp_token_idx]

            # ä¸“å®¶å¤„ç†å¹¶åŠ æƒ
            """
            expert: æ˜¯ä¸€ä¸ªä¸“å®¶æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å—ï¼‰
            expert_tokens: æ˜¯è¾“å…¥ç»™ä¸“å®¶çš„ä»¤ç‰Œï¼ˆæ•°æ®ï¼‰
            é¦–å…ˆå°†expert_tokensè¾“å…¥å¥¥expertæ¨¡å‹ä¸­å¾—åˆ°è¾“å‡ºï¼Œç„¶åå°†è¾“å‡ºè½¬ä¸º
                expert_cache.dtypeæŒ‡å®šçš„æ•°æ®ç±»å‹ï¼Œï¼ˆå¦‚åŠç²¾åº¦æµ®ç‚¹æ•°float16æˆ–è€…bfloat16ï¼‰
            ç»“æœå‚¨å­˜åœ¨expert_outä¸­

            flat_expert_weightsæ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ŒåŒ…å«äº†æ¯ä¸ªä»¤ç‰Œå¯¹åº”çš„æƒé‡ï¼Œ(é—¨æ§ç½‘ç»œäº§ç”Ÿçš„æƒé‡)
            start_idxå’Œend_idxå®šä¹‰äº†å½“å‰ä¸“å®¶å¤„ç†çš„ä»¤ç‰Œåœ¨idxsä¸­çš„èŒƒå›´
            flat_expert_weight[idxs[start_idx:end_idx]]ä¼šé€‰å–å‡ºä¸€ä¸ªæƒé‡å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸expert_outçš„ç¬¬ä¸€ç»´ç›¸åŒï¼Œ
            """
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            expert_out.mul_(flat_expert_weights[idxs[start_idx: end_idx]])

            # ç´¯åŠ åˆ°è¾“å‡ºç¼“å­˜
            expert_cache.scatter_add_(
                0,
                exp_token_idx.view(-1,1).repeat(1,x.shape[-1]),
                expert_out
            )

            return expert_cache
    
class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        """æ³¨æ„åŠ›å¤´å‚æ•°è®¾ç½®
        è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ç§æ¯ä¸ªå¤´çš„ç»´åº¦
        ç¤ºä¾‹ï¼šhidden_size= 768,num_attention_heads=12,head_dim=64
        """
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size

        # åŸºç¡€ä¼ é€’çš„å±æ€§ï¼ˆè¿™äº›æ˜¯æˆ‘çœ‹å¼•ç”¨è¿™ä¸ªç±»çš„åœ°æ–¹æ²¡æœ‰ç±»å‹æç¤ºåŠ åˆ°è¿™é‡Œçš„ï¼Œåœ¨è¿™ä¸ªç±»é‡Œæ²¡æœ‰ç”¨ï¼‰
        self.vocab_size = config.vocab_size
        self.num_hidden_layers = config.num_hidden_layers
        self.dropout = config.dropout
        self.rms_norm_eps = config.rms_norm_eps
        self.rope_scaling = config.rope_scaling
        self.rope_theta = config.rope_theta
        self.max_position_embeddings = config.max_position_embeddings

        self.head_dim = config.hidden_size // config.num_attention_heads

        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        self.self_attn = Attention(config)
        # å°†åˆ›å»ºåˆå§‹çš„å½’ä¸€åŒ–å±‚
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size,eps=config.rms_norm_eps)

        self.layer_id = layer_id
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states,position_embeddings, past_key_value=None,use_cache=False, attention_mask=None):
        """æ®‹å·®è¿æ¥è¯¦è§£
        ä¹Ÿå°±æ˜¯è¯´ç¬¬ä¸€æ¬¡ï¼ˆ residual = hidden_statesï¼‰çš„æ‰€è°“'æ®‹å·®'ï¼Œæ˜¯åŸå§‹çš„æ•°æ®ï¼ˆæˆ‘è®°ä¸ºaï¼‰ï¼Œ
        ç¬¬äºŒæ¬¡ï¼ˆ  hidden_states +=residualï¼‰
        å˜åŒ–åçš„æ®‹å·®æ˜¯åŸå§‹çš„æ•°æ®åŠ ä¸Šæ³¨æ„åŠ›å±‚è¾“å‡ºçš„æ•°æ®ï¼ˆå˜åŒ–çš„éƒ¨åˆ†ï¼‰ï¼ˆæˆ‘è®°ä¸ºbï¼‰ï¼Œ
        å†å°†è¿™ä¸ªåŸå§‹è¾“å…¥åŠ ä¸Šæ³¨æ„åŠ›å±‚å˜åŒ–åçš„å€¼ï¼ˆæˆ‘è®°ä¸ºa+bå€¼ä¸ºcï¼‰ï¼Œ
        è¿›è¡Œä¸€æ¬¡çº¿æ€§å˜åŒ–ï¼ˆc+mlp(c)ï¼‰è¾“å‡ºç»™ä¸‹ä¸ªå—çš„è¾“å…¥å€¼ï¼ˆæœ¬æ¬¡æˆ‘è®°ä¸ºd,)
        è¿™ä¸ªdå°±æ˜¯ä¸‹ä¸€ä¸ªå—çš„aäº†ï¼Œ
        ç”±äºåˆå§‹çš„ä¿¡æ¯å¾ˆå¤§éƒ¨åˆ†éƒ½æ˜¯çº¿æ€§å˜åŒ–ä¼ é€’çš„ï¼Œæ‰€ä»¥åˆ°æœ€åé¢æœ€åé¢çš„è¾“å‡ºå±‚çš„æ—¶å€™
        å’Œé¢„æœŸå€¼æ¯”è¾ƒçš„æ—¶å€™å¾ˆå¤§ä¸€éƒ¨åˆ†ä¿¡æ¯å°±å¯èƒ½ä¿ç•™ä¸‹æ¥ï¼Œ
        æ‰€ä»¥æˆ‘ç†è§£çš„å°±æ˜¯å­¦ä¹ çš„æ˜¯æ‰€è°“æ”¹å˜äº†ä»€ä¹ˆè€Œä¸æ˜¯å®Œå…¨å˜åŒ–

        a (åˆå§‹è¾“å…¥) â†’ 
        b (æ³¨æ„åŠ›è¾“å‡º) â†’ 
        c = a + b (ç¬¬ä¸€æ¬¡æ®‹å·®è¿æ¥) â†’ 
        d = c + mlp(c) (ç¬¬äºŒæ¬¡æ®‹å·®è¿æ¥ï¼Œä¹Ÿæ˜¯æœ¬å—è¾“å‡º) â†’ 
        ä¸‹ä¸€ä¸ªå—çš„ a
        """
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states),  # Pre-Norm: å…ˆå½’ä¸€åŒ–ä¸Šä¸€å±‚ä¼ é€’ä¸‹æ¥çš„æ•°æ®
            position_embeddings,
            past_key_value,
            use_cache,
            attention_mask
        ) 
        hidden_states +=residual   # æ®‹å·®è¿æ¥
        # å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        """
        å¤„ç†æµç¨‹ï¼š
        å½’ä¸€åŒ–å½“å‰çŠ¶æ€
        é€šè¿‡MLP/MoEå‰é¦ˆç½‘ç»œ
        æ®‹å·®è¿æ¥åŠ åˆ°åŸå§‹çŠ¶æ€
        """
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))

        return hidden_states, present_key_value

class MiniMindModel(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.num_hidden_layers = config.num_hidden_layers
        # æ ¸å¿ƒç»„ä»¶
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        """è¯åµŒå…¥å±‚
        ä½œç”¨ï¼šå°†è¾“å…¥çš„token idè½¬æ¢ä¸ºå¯†é›†å‘é‡è¡¨ç¤º
        æœ¬è´¨ï¼š
        å®ƒæ˜¯ä¸€ä¸ªå¯æŸ¥è¯¢çš„å­—å…¸ï¼Œå…¶é”®æ˜¯æ•´æ•°ï¼ˆç´¢å¼•ï¼Œä»£è¡¨å•è¯IDï¼‰ï¼Œå€¼æ˜¯å›ºå®šå¤§å°çš„å‘é‡ï¼ˆè¯å‘é‡ï¼‰ã€‚
        config.vocab_sizeï¼š å­—å…¸çš„å¤§å°ã€‚æ¯”å¦‚ 50000ï¼Œè¡¨ç¤ºæ¨¡å‹è®¤è¯† 50000 ä¸ªä¸åŒçš„å•è¯/å­è¯ã€‚
        config.hidden_sizeï¼š æ¯ä¸ªè¯å‘é‡çš„ç»´åº¦ã€‚æ¯”å¦‚ 512ï¼Œè¡¨ç¤ºæ¯ä¸ªå•è¯ç”¨ä¸€ä¸ª 512 ç»´çš„å‘é‡æ¥è¡¨ç¤ºã€‚
        
        è¾“å…¥ï¼š[batch_size,seq_len](æ•´æ•°token ID)
        è¾“å‡ºï¼š[batch_size,seq_len,hidden_size](æµ®ç‚¹æ•°å‘é‡)
        """
        self.dropout = nn.Dropout(config.dropout) 
        """åµŒå…¥åçš„dropout"""
        self.layers = nn.ModuleList([
            # 8ä¸ªtransformerå—
            MiniMindBlock(l,config) for l in range(self.num_hidden_layers)
        ])
        """Transformerå±‚å †å 
        ä½œç”¨ï¼šåˆ›å»ºå¤šä¸ªTransformerå—ï¼Œå½¢æˆæ·±åº¦ç½‘ç»œï¼Œ
        ä¸€èˆ¬è€Œè¨€æ¯ä¸ªTransformerå—é…ç½®ç›¸åŒï¼Œå…±åŒä½¿ç”¨åŒä¸€ä¸ªé…ç½®
        """
        self.norm = RMSNorm(config.hidden_size,eps=config.rms_norm_eps)
        """æœ€ç»ˆå±‚å½’ä¸€åŒ–
        ç”Ÿæˆåˆå§‹åŒ–çš„å¯è®­ç»ƒå½’ä¸€åŒ–å±‚
        """

        freqs_cos,freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads,
            end=config.max_position_embeddings,
            rope_base=config.rope_theta,
            rope_scaling=config.rope_scaling
        )
        """æ—‹è½¬ä½ç½®ç¼–ç é¢„è®¡ç®—
        é¢„è®¡ç®—ä¼˜åŒ–ï¼š åœ¨åˆå§‹åŒ–æ—¶å€™è®¡ç®—æ‰€æœ‰ä½ç½®çš„ä½ç½®ç¼–ç ï¼Œé¿å…é‡å¤è®¡ç®—
        register_buffer: å°†ä½ç½®ç¼–ç æ³¨å†Œä¸ºæ¨¡å‹ç¼“å†²åŒºï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        persistent=False: ä¸ä¿å­˜åˆ° state_dict,èŠ‚çœå­˜å‚¨ç©ºé—´
        """  

        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer('freqs_sin',freqs_sin,persistent=False)
    
    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor]=None,
                past_key_values: Optional[List[Tuple[torch.Tensor,torch.Tensor]]]=None,
                use_cache: bool = False,
                **kwargs ):
          
          batch_size,seq_lenth = input_ids.shape
          if hasattr(past_key_values,'layers'): past_key_values = None #å…¼å®¹æ€§å¤„ç†
          #   é»˜è®¤å€¼è®¾ç½®ï¼Œå¦‚æœpast_key_valuesä¸ºNoneï¼ˆä¸Šé¢å…¼å®¹å¤„ç†åçš„æƒ…å†µï¼‰ï¼Œé‡æ–°åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨
          #   åˆ—è¡¨é•¿åº¦ = æ¨¡å‹å±‚æ•°ï¼ˆlen(self.layers)ï¼‰
          #   æ¯ä¸ªå…ƒç´ åˆå§‹åŒ–ä¸ºæ¬¸None

          """ æ£€æŸ¥past_key_valuesæ˜¯å¦å…·æœ‰layerså±æ€§
          æ£€æŸ¥past_key_valuesæ˜¯å¦å…·æœ‰layerså±æ€§
          èƒŒæ™¯ï¼šæŸäº›ç‰ˆæœ¬çš„Transformerå®ç°å¯èƒ½å°†past_key_valueså°è£…å­å•Šå¯¹è±¡ä¸­è€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨åˆ—è¡¨
          æ•°æ®çŠ¶æ€ï¼šå¦‚æœpast_key_valuesæ˜¯å¯¹è±¡è€Œéåˆ—è¡¨ï¼Œè¯´æ˜æ•°æ®ç»“æ„ä¸å…¼å®¹
          é‡åˆ°ä¸è®¤è¯†çš„past_key_valuesæ ¼å¼æ—¶ï¼Œæ”¾å¼ƒä½¿ç”¨å†å²ç¼“å­˜ï¼Œé‡æ–°å¼€å§‹

          æ‰§è¡Œç¤ºä¾‹ï¼š
          å‡è®¾æ¨¡å‹æœ‰8å±‚ï¼Œå¯èƒ½ä¼šæœ‰ä¸€ä¸‹æ‰§è¡Œè·¯å¾„
          æƒ…å†µ1ï¼šé¦–æ¬¡è°ƒç”¨ï¼ˆæ— å†å²ç¼“å­˜ï¼‰
          past_key_values = None
            ->æ‰§è¡Œåï¼š past_key_values = [None, None, None, None, None, None, None, None]
          æƒ…å†µ2ï¼šæ­£å¸¸ç»­ä¼ ï¼ˆæœ‰æ­£ç¡®æ ¼å¼çš„ç¼“å­˜ï¼‰
          past_key_values = [(k1,v1),(k2,v2),(k3,v3),(k4,v4)(k5,v5),(k6,v6),(k7,v7)]
            ->æ‰§è¡Œåï¼š â†’ ä¿æŒä¸å˜ï¼špast_key_values = åŸå§‹ç¼“å­˜åˆ—è¡¨
          æƒ…å†µ3ï¼šæ ¼å¼ä¸å…¼å®¹çš„ç¼“å­˜
            past_key_values = æŸä¸ªå…·æœ‰layerså±æ€§çš„å¯¹è±¡
              ->å…ˆé‡ç½®ä¸ºNoneï¼Œå†é‡æ–°åˆ›å»ºæ–°åˆ—è¡¨
          """
          past_key_values = past_key_values or [None] * len(self.layers)
          start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0
          # è®¡ç®—èµ·å§‹ä½ç½®ï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡  ï¼‰   
          """è®¡ç®—èµ·å§‹ä½ç½®
          æ¡ä»¶åˆ¤æ–­: past_key_values[0] is None
          ç›®çš„ï¼šæ£€æŸ¥ç¬¬ä¸€å±‚æ˜¯å¦å­˜åœ¨KVç¼“å­˜
          é€»è¾‘ï¼šå¦‚æœç¬¬ä¸€å±‚æœ‰ç¼“å­˜ï¼Œè¯´æ˜ä¸æ˜¯ç¬¬ä¸€æ¬¡å¤„ç†ï¼Œéœ€è¦ç»­ä¼ ï¼Œå¦åˆ™ä»å¤´å¼€å§‹
          
          2ã€KVç¼“å­˜æ•°æ®ç»“æ„
          å‡è®¾past_key_valuesçš„ç»“æ„ï¼š
            past_key_values = [
                (key_tensor_layer0, value_tensor_layer0), # ç¬¬0å±‚
                (key_tensor_layer1, value_tensor_layer1), # ç¬¬1å±‚
                ...å…¶ä»–å±‚
            ]
            æ•°å€¼ç¤ºä¾‹
            å‡è®¾ï¼šbatch_size =1
                  å·²å¤„ç†çš„åºåˆ—é•¿åº¦ä¸º = 3
                  æ³¨æ„åŠ›å¤´2
                  æ¯ä¸ªå¤´çš„ç»´åº¦=4
                  æ¨¡å‹å±‚æ•°=3
            ç¬¬0å±‚çš„KVç¼“å­˜
            # Key å¼ é‡ (å½¢çŠ¶: [1, 2, 3, 4])
            key_layer0 = torch.tensor([[
                [[0.1, 0.2, 0.3, 0.4],   # å¤´0ï¼Œtoken0çš„ç‰¹å¾
                [0.5, 0.6, 0.7, 0.8],   # å¤´0ï¼Œtoken1çš„ç‰¹å¾  
                [0.9, 1.0, 1.1, 1.2]],  # å¤´0ï¼Œtoken2çš„ç‰¹å¾
                
                [[1.1, 1.2, 1.3, 1.4],   # å¤´1ï¼Œtoken0çš„ç‰¹å¾
                [1.5, 1.6, 1.7, 1.8],   # å¤´1ï¼Œtoken1çš„ç‰¹å¾
                [1.9, 2.0, 2.1, 2.2]]   # å¤´1ï¼Œtoken2çš„ç‰¹å¾
            ]])

            # Value å¼ é‡ (å½¢çŠ¶: [1, 2, 3, 4])  
            value_layer0 = torch.tensor([[
                [[2.1, 2.2, 2.3, 2.4],   # å¤´0ï¼Œtoken0çš„ç‰¹å¾
                [2.5, 2.6, 2.7, 2.8],   # å¤´0ï¼Œtoken1çš„ç‰¹å¾
                [2.9, 3.0, 3.1, 3.2]],  # å¤´0ï¼Œtoken2çš„ç‰¹å¾
                
                [[3.1, 3.2, 3.3, 3.4],   # å¤´1ï¼Œtoken0çš„ç‰¹å¾
                [3.5, 3.6, 3.7, 3.8],   # å¤´1ï¼Œtoken1çš„ç‰¹å¾
                [3.9, 4.0, 4.1, 4.2]]   # å¤´1ï¼Œtoken2çš„ç‰¹å¾
            ]])
          å½¢çŠ¶è§£æ
          # è®¿é—®ç¬¬0å±‚çš„keyå¼ é‡
            key_tensor = past_key_values[0][0]  # å½¢çŠ¶: [1, 2, 3, 4]

            # å„ç»´åº¦å«ä¹‰ï¼š
            print(f"batch_size: {key_tensor.shape[0]}")      # 1
            print(f"num_heads: {key_tensor.shape[1]}")       # 2  
            print(f"seq_len: {key_tensor.shape[2]}")         # 3 â† è¿™å°±æ˜¯start_posè¦çš„å€¼!
            print(f"head_dim: {key_tensor.shape[3]}")        # 4


          3ã€å½¢çŠ¶åˆ†æ
          past_key_values[0][0].shape[1]  #å–ç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªå…ƒç´ keyçš„ç¬¬äºŒç»´

          past_key_values[0]   ç¬¬0å±‚çš„(key,value)å…ƒç»„
          past_key_values[0][0]   ç¬¬0å±‚çš„keyå¼ é‡
          .shape[1]  keyå¼ é‡çš„ç¬¬äºŒç»´ï¼ˆåºåˆ—é•¿åº¦ï¼‰
          å¼ é‡å½¢çŠ¶(shape):(batch_size,seq_len,num_heads,head_dimï¼‰
            ç¬¬ä¸€ç»´: batch_size,
            ç¬¬äºŒç»´ï¼šseq_len(!!è¿™ä¸ªå°±æ˜¯æˆ‘ä»¬éœ€è¦çš„)
            ç¬¬ä¸‰ç»´ï¼šæ³¨æ„åŠ›å¤´æ•°
            ç¬¬å››ç»´ï¼š æ¯ä¸ªå¤´çš„ç»´åº¦
        
          è®¡ç®—ç¤ºä¾‹
          æƒ…å†µ1ï¼šé¦–æ¬¡æ¨ç†ï¼ˆæ— ç¼“å­˜ï¼‰
            past_key_values[0]=None
            -> start_pos = 0
          æƒ…å†µ2ï¼šç»­ä¼ æ¨ç†ï¼ˆæœ‰ç¼“å­˜ï¼‰
            past_key_values[0] = (key_tensor,value_tensor)
            key_tensor.shape = (1, 128, 8, 64) #batch = 1ï¼Œå·²å¤„ç†128ä¸ªtoken
            -> start_pos = 128

          """
          
          hidden_states = self.dropout(self.embed_tokens(input_ids))
          """æœ€åˆçš„hidden_states"""
         
          position_embeddings = (
              self.freqs_cos[start_pos:start_pos+seq_lenth],
              self.freqs_sin[start_pos:start_pos+seq_lenth]  
          ) 
          # ä»start_poså¼€å§‹æˆªå–
          """
          å®é™…åœºæ™¯
          è¾“å…¥ï¼š"Hello"
          start_pos = 0
          ä½ç½®ç¼–ç ï¼šcos[0:5], sin[0,5] #å¤„ç†5ä¸ªtoken
          ç»­ä¼ ç”Ÿæˆï¼š
          å·²å¤„ç†ï¼šHello world (11ä¸ªtoken)
          æ–°è¾“å…¥ï¼šhow are (3ä¸ªtoken)
          start_pos = 11
          ä½ç½®ç¼–ç ï¼šcos[11:14] sin[11,14]#ä»ç¬¬11ä¸ªä½ç½®å¼€å§‹
          """

          presents = []
          for layer_idx, (layer,past_key_value) in enumerate(zip(self.layers, past_key_values)):
              #   layerå°±æ˜¯æ¯ä¸€ä¸ªMiniMindBlockæ„å»ºçš„å±‚
              """hidden_statesæ˜¯è¢«layerå±‚æ›´æ–°åçš„"""
              hidden_states,present = layer(
                  hidden_states,
                  position_embeddings,
                  past_key_value=past_key_value,
                  use_cache=use_cache,
                  attention_mask=attention_mask
              )

              presents.append(present)

          hidden_states = self.norm(hidden_states)

          aux_loss = sum(
            layer.mlp.aux_loss
            for layer in self.layers
            if isinstance(layer.mlp,MOEFeedForward)
          )

          return hidden_states, presents, aux_loss

class MiniMindForCausalLM(PreTrainedModel,GenerationMixin):
    config_class = MiniMindConfig

    def __init__(self, config: MiniMindConfig=None):
        self.config = config or MiniMindConfig()
        super().__init__(self.config)
        self.model = MiniMindModel(self.config)
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size,bias=False)
        self.model.embed_tokens.weight = self.lm_head.weight
        self.OUT = CausalLMOutputWithPast()

    def forward(self,
                input_ids: Optional[torch.Tensor]=None,
                attention_mask: Optional[torch.Tensor]=None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]]=None,
                use_cache:bool=False,
                logits_to_keep: Union[int,torch.Tensor]=0,
                **args  
                ):
        h, past_kvs,aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )

        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep,int) else logits_to_keep
        logits = self.lm_head(h[:,slice_indices,:])
        self.OUT.__setitem__('last_hidden_state',h)
        self.OUT.__setitem__('logits',logits)
        self.OUT.__setitem__('aux_loss',aux_loss)
        self.OUT.__setitem__('past_key_values',past_kvs)
        return self.OUT
