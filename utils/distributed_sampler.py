import torch
import torch.distributed as dist

# class DistributedSampler:
#     def __init__(self, dataset, num_replicas=None, rank=None):
#         self.dataset = dataset
#         self.num_replicas = dist.get_world_size()  # æ€»GPUæ•°
#         self.rank = rank                # å½“å‰GPUç¼–å·
#         self.epoch = 0
        
#     def __iter__(self):
#         # 1. æ ¹æ®epochè®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°æ€§
#         g = torch.Generator()
#         g.manual_seed(self.epoch)
        
#         # 2. ç”Ÿæˆæ‰€æœ‰æ•°æ®çš„éšæœºæ’åˆ—
#         indices = torch.randperm(len(self.dataset), generator=g).tolist()
        
#         # 3. æ ¹æ®rankåˆ†é…æ•°æ®ç‰‡æ®µ
#         indices = indices[self.rank::self.num_replicas]
#         return iter(indices)
    

from transformers import AutoTokenizer
import os
import json

def comprehensive_tokenizer_analysis(tokenizer_path):
    print("ğŸ” å¼€å§‹åˆ†è¯å™¨åˆ†æ...")
    
    # 1. æ£€æŸ¥æ–‡ä»¶
    print("\n1. ğŸ“ æ–‡ä»¶æ£€æŸ¥:")
    for file in os.listdir(tokenizer_path):
        file_path = os.path.join(tokenizer_path, file)
        size = os.path.getsize(file_path)
        print(f"   {file}: {size:,} bytes")
    
    # 2. åŠ è½½åˆ†è¯å™¨
    print("\n2. ğŸš€ åŠ è½½åˆ†è¯å™¨...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        print("   âœ… åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return
    
    # 3. åŸºæœ¬ä¿¡æ¯
    print("\n3. ğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"   - ç±»å‹: {type(tokenizer).__name__}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size:,}")
    print(f"   - æ¨¡å‹æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
    
    # 4. ç‰¹æ®Šä»¤ç‰Œ
    print("\n4. ğŸ¯ ç‰¹æ®Šä»¤ç‰Œ:")
    special_tokens = [
        ('bos_token', 'å¼€å§‹ä»¤ç‰Œ'),
        ('eos_token', 'ç»“æŸä»¤ç‰Œ'), 
        ('pad_token', 'å¡«å……ä»¤ç‰Œ'),
        ('unk_token', 'æœªçŸ¥ä»¤ç‰Œ'),
        ('mask_token', 'æ©ç ä»¤ç‰Œ')
    ]
    
    for token_name, desc in special_tokens:
        token = getattr(tokenizer, token_name, None)
        token_id = getattr(tokenizer, token_name + '_id', None)
        status = "âœ…" if token else "âŒ"
        print(f"   - {desc}({token_name}): {status} {token} (ID: {token_id})")
    
    # 5. æµ‹è¯•åˆ†è¯
    print("\n5. ğŸ§ª åˆ†è¯æµ‹è¯•:")
    test_samples = [
        "Hello world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "I love programming.",
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚"
    ]
    
    for sample in test_samples:
        tokens = tokenizer.tokenize(sample)
        input_ids = tokenizer.encode(sample, add_special_tokens=False)
        print(f"   '{sample}'")
        print(f"     â†’ Tokens: {tokens}")
        print(f"     â†’ IDs: {input_ids}")
    
    # 6. è¯æ±‡è¡¨æ ·æœ¬
    print("\n6. ğŸ“š è¯æ±‡è¡¨æ ·æœ¬:")
    vocab = tokenizer.get_vocab()
    print(f"   - æ€»tokenæ•°: {len(vocab):,}")
    
    # å‰5ä¸ª
    print("   - å‰5ä¸ªtoken:")
    for i, (token, token_id) in enumerate(list(vocab.items())[:5]):
        print(f"     {token_id}: '{token}'")
    
    # æœ€å5ä¸ª  
    print("   - æœ€å5ä¸ªtoken:")
    for i, (token, token_id) in enumerate(list(vocab.items())[-5:]):
        print(f"     {token_id}: '{token}'")

# è¿è¡Œåˆ†æ
comprehensive_tokenizer_analysis('../model/')

# ä½¿ç”¨è¯Šæ–­
if __name__ == "__main__":
        comprehensive_tokenizer_analysis('../model/')
    